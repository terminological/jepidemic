---
title: "Appendix C - Bayesian growth rate method"
author: "Rob Challen"
date: '`r format(Sys.Date(), "%d-%m-%Y")`'
output: 
  pdf_document :
    fig_caption: yes
header-includes:
 \usepackage{float}
 \usepackage{mathtools}
 \usepackage{amsmath}
 \DeclareRobustCommand{\[}{\begin{equation*}}
 \DeclareRobustCommand{\]}{\end{equation*}}
 \newcounter{tagno}
 \setcounter{tagno}{0}
 \let\amsmathtag\tag
 \renewcommand{\tag}[1]{\amsmathtag{\thetagno} \label{#1} \stepcounter{tagno}}
 \floatplacement{figure}{H}    
knit: (function(inputFile, encoding,...) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "~/Dropbox/sarscov2/r-estimation-methodology", output_file=paste0('bayesian-growth-rate-',Sys.Date(),'.pdf')) })
fig_width: 7
fig_height: 5
out.width: "100%"
bibliography: jepidemic.bib
csl: jepidemic.csl
vignette: >
  %\VignetteIndexEntry{Cori method}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)


here::i_am("vignettes/bayesian-growth-rate.Rmd")
source(here::here("vignettes/common-setup.R"))

```

## Estimation of the incidence of infection

Assume $I_0, I_1, \dots, I_t$ is a time series of infection counts, assumed to be drawn from some discrete probability distribution with expected value $\overline{I_t}$. Assume $I_t$ is a Poisson distributed quantity, with a rate parameter which is a function of time:


$$
\begin{aligned}
E[I_t] &= \lambda_t \\
\end{aligned}
$$

Assuming $\lambda_t$ is constant over a short time period $[t-\tau;t+\tau]$ then by the definition of the Poisson distribution:


$$
\begin{aligned}
P(I_{t-\tau},\dots,I_{t+\tau}|\lambda_t) &= \prod_{s=t-\tau}^{t+\tau}\frac{e^{-\lambda_t}\lambda_t^{I_s}}{I_s!} \\
\end{aligned}
$$

If we assume the Poisson rate parameter, $\lambda_t$, to be gamma distributed with shape parameter $\alpha$ and rate parameter $\beta$, and if $n = 2\tau+1$ then we can use a Bayesian framework to derive posterior estimates of the distribution of the Poisson rate, labelled $\alpha'$ and $\beta'$ conditioned on the data we observe over $[t-\tau;t+\tau]$:

$$
\begin{aligned}
P(I_{t-\tau},\dots,I_{t+\tau}|\lambda_t) &= \frac{e^{-n\lambda_t}\lambda_t^{\sum{I_s}}}{\prod_{s=t-\tau}^{t+\tau}I_s!} \\
P(\lambda_{t}) &= \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda_{t}^{\alpha-1}e^{-\beta\lambda_{t}} \\
P(\lambda_t|I_{t-\tau},\dots,I_{t+\tau}) &= \frac{P(I_{t-\tau},\dots,I_{t+\tau}|\lambda_t)P(\lambda_{t})}{P(I_{t-\tau},\dots,I_{t+\tau})}\\
P(\lambda_t|I_{t-\tau},\dots,I_{t+\tau}) &= \frac{e^{-n\lambda_t}\lambda_t^{\sum{I_s}}}{\prod_{s=t-\tau}^{t+\tau}I_s!}\frac{\beta^\alpha}{\Gamma(\alpha)} \lambda_{t}^{\alpha-1}e^{-\beta\lambda_{t}}\\
P(\lambda_t|I_{t-\tau},\dots,I_{t+\tau}) &\propto \lambda_t^{\sum{I_s+\alpha-1}}e^{-(2\tau+1+\beta)\lambda_t} \\
P(\lambda_t|I_{t-\tau},\dots,I_{t+\tau}) &\sim \Gamma\big(\sum_{t-\tau}^{t+\tau}{I_s}+\alpha, 2\tau+1+\beta\big)\\
\alpha' &= \alpha+\sum_{t-\tau}^{t+\tau}{I_s}\\
\beta' &= 2\tau+1+\beta
\end{aligned}
$$

The posterior estimate of the Poisson rate $\lambda$ is gamma distributed by definition. An estimate of the likely value of $I_t$ ($\overline{I_t}$) given the observed data is given by the posterior predictive distribution:

$$
\begin{aligned}
\overline{I_t} &\sim NegBin\Big(\alpha',\frac{1}{\beta'+1}\Big)\\
E(\overline{I_t}|I_{t-\tau},\dots,I_{t+\tau}) &= \frac{\alpha'}{\beta'} \\
V(\overline{I_t}|I_{t-\tau},\dots,I_{t+\tau}) &= \alpha'\Big(\frac{\beta'+1}{\beta'^2}\Big)
\end{aligned}
$$

<!-- There is probably something interesting we can do with the $P(\overline{I_t}|I_{t-\tau} \dots I_{t+\tau})$ to detect importation or anomaly events -->

N.B. need to talk about prior selection.

# Estimation of the growth rate

The exponential growth rate $r_t$ is the gradient of the logarithm of $I$ ($\frac{d}{dt}log(\overline{I_t})$) with respect to time. We approximate this as the difference between two estimates of the true value of $I$ seperated by a small time period $2m$:

$$
\begin{aligned}
r_t &\approx \frac{1}{2m}(log(E(I_{t+m}))-log(E(I_{t-m}))) \\
r_t &= \frac{1}{2m}log\frac{\lambda_{t+m}}{\lambda_{t-m}}
\end{aligned}
$$
Given that $I$ is assumed to be Poisson distributed the expected value is $\lambda_t$. If we further define $y = g(r)$ such that:

$$
\begin{aligned}
y = g(r_t) &= e^{2\tau r_t}\\
r_t = g^{-1}(y) &= \frac{1}{2\tau}log(y)\\
\end{aligned}
$$

Then we can express the distribution of $g(r)$ in terms of a ratio of Gamma distributed quantities. For simplification we have also assumed that $m=\tau$. This means that is estimating the growth rate we use two posterior estimates of the Poisson rate based on $\tau+1$ data points. The growth rate estimate is therefore based on $2\tau+1$ data points, as the central point overlaps. This is a choice as we could have used any 2 estimates which are close together, but this has the benefit of not re-using information. 

$$
\begin{aligned}
r_t &= \frac{1}{2\tau}log\frac{\lambda_{t+\tau}}{\lambda_{t-\tau}} \\
g(r_t) &\sim \Bigg(\frac{Gamma(\alpha+\sum_{s=t}^{t+2\tau}{I_s}, \beta')}{Gamma(\alpha+\sum_{r=t-2\tau}^{t}{I_r}, \beta')}\Bigg) \\
\alpha'_{\tau+} &= \alpha+\sum_{t}^{t+2\tau}{I_s}\\
\alpha'_{\tau-} &= \alpha+\sum_{t-2\tau}^{t}{I_s}\\
g(r_t) &\sim \Bigg(\frac{
Gamma(\alpha'_{\tau+}, \beta')
}{
Gamma(\alpha'_{\tau-}, \beta')
}
\Bigg)\\
\end{aligned}
$$

The ratio of 2 gammas with same rate parameter is a Beta Prime distributed quantity, which describes $g(r)$: 

https://en.wikipedia.org/wiki/Beta_prime_distribution

$$
\begin{aligned}
r_t \sim Y &= \frac{1}{2\tau}log\Big(BetaPrime\big(\alpha'_{\tau+}, \alpha'_{\tau-})\Big) \\
g(r_t) \sim X &= BetaPrime\big(\alpha+\sum_{t}^{t+2\tau}{I}, \alpha+\sum_{t-2\tau}^{t}{I}\big) \\
BetaPrime(\alpha_1, \alpha_2) &: \\
f(x) &= \frac{1}{B(\alpha_1,\alpha_2)}x^{\alpha_1-1}(1+x)^{-\alpha_1-\alpha_2} \\
F(x) &= I_{
\frac{x}{1+x}}(\alpha_1,\alpha_2)
\end{aligned}
$$
Where $I$ is the regularised beta function and $B$ is the complete beta function. The support for $g(r_t)$ is $[0 \dots \infty]$ and hence the support for $r_t$ is $[-\infty \dots \infty]$. Given that $g(r_t)$ can be differentiated and is a strictly increasing function we can apply the following transformation:

https://www.statlect.com/fundamentals-of-probability/functions-of-random-variables-and-their-distribution
https://www.probabilitycourse.com/chapter4/4_1_3_functions_continuous_var.php

$$
\begin{aligned}
Y &= \frac{1}{2\tau}log(X) \\
g(x) &= \frac{1}{2\tau}log(x) \\
g^{-1}(y) &= e^{2\tau y} \\
\frac{dg^{-1}}{dy} &= 2\tau e^{2\tau y}\\
F_Y(y) &= F_X(g^{-1}(y)) \\
f_Y(y) &= f_X(g^{-1}(y)) \frac{dg^{-1}(y)}{dy}
\end{aligned}
$$

Which gives us a probability density function for an estimate of $r_t$ based on the bayesian posterior of $\lambda_t$:
$$
\begin{aligned}
f_Y(r_t) &= f_X(g^{-1}(r_t))\frac{dg^{-1}(r_t)}{dr_t}\\
f_Y(r_t) &= 2\tau e^{2\tau r_t}f_X(e^{2\tau r_t})\\
f_Y(r_t) &=  2\tau e^{2\tau r_t}\frac{
\Big(
  e^{r_t(\alpha'_{\tau+}-1)}
  (1+e^{r_t})^{(-\alpha'_{\tau+}-\alpha'_{\tau-})}
\Big)}{B(\alpha'_{\tau+}, \alpha'_{\tau-})}\\
f_Y(r_t) &=  \frac{2\tau}{B(\alpha'_{\tau+}, \alpha'_{\tau-})}
\Big(
  e^{r_t(\alpha'_{\tau+}+2\tau-1)}
  (1+e^{r_t})^{(-\alpha'_{\tau+}-\alpha'_{\tau-})}
\Big)\\
\end{aligned}
$$

Which has the following cumulative probability function:

$$
\begin{aligned}
F_Y(r_t) &= F_Y(g^{-1}(r_t))\\
F_X(r_t) &= F_Y(e^{2\tau r_t})\\
F_X(r_t) &= I_{
\frac{e^{2\tau r_t}}{1+e^{2\tau r_t}}}(\alpha_{\tau+},\alpha_{\tau-})
\end{aligned}
$$

where $I_x(a,b)$ is the regularised beta function.

https://mathworld.wolfram.com/RegularizedBetaFunction.html
https://commons.apache.org/proper/commons-math/javadocs/api-3.3/org/apache/commons/math3/special/Beta.html
https://en.wikipedia.org/wiki/Beta_prime_distribution#Properties

The quantile function for $g(r_t)$ is similarly transformed backwards to give a quantile estimate for $r_t$, and we can sample from $Y$ and transform using $g^{-1}(y)$ to get unbiased samples of $r_t$.

# Reproduction number estimates from Poisson rate

If we continue to assume $I_t \sim Poisson(\lambda_t)$ and an estimate of $\lambda_t$ is available. $\omega_1, \omega_2, \dots, \omega_s$ is another probability distribution, the infectivity profile, that defines the likelihood that a case infected at time $t$ resulted from a case infected between the times $t-s$ and $t-s+1$. This definition implies that $\omega_{s \leq 0} = 0$ as that applies to infections in the future, and that the discrete time measure $s$ here represents the upper bound of the equivalent continuous unit time interval, rather than, for example, the middle of the interval. 

<!-- $$ -->
<!-- \overline{I_t} = \lambda_t\\ -->
<!-- R_t = \frac{\text{secondary cases}}{\text{contributing primary cases}} \\ -->

<!-- R_t = \frac{\lambda_t}{\sum_{s=1}^t \lambda_{t-s}\omega_s} -->
<!-- $$ -->

<!-- since the denominator is all poisson distributions we can combine to give@ -->

<!-- $$ -->
<!-- R_t \sim \frac{Poisson(\lambda_t)}{Poisson(\sum_{s=1}^t \lambda_{t-s}\omega_s)} -->
<!-- $$ -->

<!-- N.b. There is a finite probability of extinction in which case the denominator is zero: -->

<!-- $$ -->

<!-- P(\text{extinction}_t) = P(\text{contributing primary cases} = 0)\\ -->
<!-- P(\text{extinction}_t) = P(Poisson(\sum_{s=1}^t \lambda_{t-s}\omega_s) = 0)\\ -->
<!-- P(\text{extinction}_t) = \Big(\sum_{s=1}^t \lambda_{t-s}\omega_s\Big)e^{-\Big(\sum_{s=1}^t \lambda_{t-s}\omega_s\Big)} -->
<!-- $$ -->


<!-- Although PDF is not defined in the general case Cumulative probability is defined: -->

<!-- https://stats.stackexchange.com/questions/10951/what-is-the-distribution-of-the-ratio-of-two-poisson-random-variables -->

<!-- $$ -->
<!-- \mathbb{P}\left[\frac{X}{Y} \leq r \right] := \mathbb{P}\left[X \leq r Y\right]\\ -->
<!-- = \sum_{y = 0}^\infty \sum_{x=0}^{\left\lfloor ry \right\rfloor} \frac{\lambda_{2}^y }{y!}e^{-\lambda_2} \frac{\lambda_{1}^x }{x!}e^{-\lambda_1} -->
<!-- $$ -->
<!-- And the PDF : -->
<!-- "The density follows from the Radon-Nykodym theorem." -->
<!-- https://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem -->
<!-- But this is mind-boggling -->


<!-- There is a bayesian estimator of this ratio: ( I think it is the "simple ratio" bit of this paper:) -->
<!-- https://arxiv.org/pdf/astro-ph/0606247.pdf -->
<!-- but it maybe needs to be reworked here. There is no closed form solution to it.  -->


<!-- The wikipedia entry has forms for mean and variance of this ratio: -->
<!-- https://en.wikipedia.org/wiki/Ratio_distribution#Poisson_and_truncated_Poisson_distributions -->

As before we define the backward-looking effective reproduction number $R_t$ as the inverse ratio of the number of primary infections associated with secondary infections observed at time $t$, this is known as the instantaneous reproduction number. 

$$
R_t = \frac{\overline{I_t}}{\sum_{s=1}^t \overline{I_{t-s}}\omega_s}
$$
The posterior distribution of $\lambda_t$ is an estimate of the $\overline{I_t}$ distribution and therefore . 

$$
R_t = \frac{\lambda_t}{\sum_{s=1}^t \lambda_{t-s}\omega_s}
$$

assuming the posterior distribution of $\lambda_t \sim Gamma(\alpha',\beta')$ as described above., we consider the denominator as the sum of scaled gamma distributions, and we can say the distribution of the denominator is of the form:

$$
\lambda_{t-s}\omega_s \sim Gamma\big(\alpha'_{t-s}, \frac{\beta'_{t-s}}{\omega_s}\big)\\ 
$$

A commonly used approximation for a sum of gamma distributed variables is as another gamma distribution with the same first and second moments as described by in Cove et al [CITE]. In the case of a set of gamma distributions, with parameters $\alpha_i$ and $\beta_i$, an approximation of the sum is another gamma distribution with parameters $\alpha_{sum}$ and $\beta_{sum}$:

$$
\begin{aligned}
E(X_i) &= \frac{\alpha_i}{\beta_i}\\
V(X_i) &= \frac{\alpha_i}{\beta_i^2}\\
E(X_{sum}) &= \sum_iE(X_i) =  \sum_i\frac{\alpha_i}{\beta_i}\\
V(X_{sum}) &= \sum_iV(X_i)= \sum_i\frac{\alpha_i}{\beta_i^2}\\
\alpha_{sum} &= \frac{E(X_{sum})^2}{V(X_{sum})} = \frac{\Big(\sum_i\frac{\alpha_i}{\beta_i}\Big)^2}{\sum_i\frac{\alpha_i}{\beta_i^2}}\\
\beta_{sum} &= \frac{E(X_{sum})}{V(X_{sum})} = \frac{\sum_i\frac{\alpha_i}{\beta_i}}{\sum_i\frac{\alpha_i}{\beta_i^2}}\\
\end{aligned}
$$

1. Covo S, Elalouf A. A novel single-gamma approximation to the sum of independent gamma variables, and a generalization to infinitely divisible distributions. Electronic Journal of Statistics [Internet]. 2014 Jan [cited 2021 Nov 25];8(1):894â€“926. Available from: https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-8/issue-1/A-novel-single-gamma-approximation-to-the-sum-of-independent/10.1214/14-EJS914.full

The moment matching approximation for the sum of gamma distributions can be empirically tested. Here we demonstrate its use on a random set of five gamma distributions show in panel A and the sum of a million random draws from each of these probability distributions in panel B. In panel B the red line is the predicted Gamma distribution based on the estimator. Further validation of the estimator is warranted but in simple cases the estimator is observed to perform well.

```{r}
seed = 101
means = runif(5,min = 1,max=2)
sds = runif(5,min=0.1,max=0.5)

alphas = (means/sds)^2
betas = means/sds^2

alphaSum = sum(alphas/betas)^2/(sum(alphas/betas^2))
betaSum = alphaSum / sum(alphas/betas)

meanSum = alphaSum/betaSum
sdSum = sqrt(alphaSum/betaSum^2)

data = tibble(alpha = alphas, beta = betas, type = rep("component",5), distribution=1:5)
pdfs = data %>% group_by_all() %>% summarise(
  x=seq(0,3,length.out = 1001), 
  p=dgamma(x,alpha,beta))

p1 = ggplot(pdfs %>% ungroup(), aes(x=x,y=p,colour=as.factor(distribution)))+geom_line()+guides(colour=guide_none())+ylab("P(x)")

hist = data %>% group_by(distribution) %>% 
  summarise(
    y = rgamma(1000000,alpha,beta),
    sample = 1:1000000
  ) %>%
  group_by(sample) %>% 
  summarise(sum_y = sum(y))


              
estim = tibble(alpha =alphaSum, beta = betaSum) %>% group_by_all() %>% summarise(
  x=seq(min(hist$sum_y),max(hist$sum_y),length.out = 2001), 
  p=dgamma(x,alpha,beta)
)

p2 = ggplot(hist,aes(x=sum_y))+geom_histogram(aes(y = 10*stat(count / sum(count))),binwidth = 0.1,fill=NA,colour="grey50")+
  geom_line(data=estim, mapping = aes(x=x,y=p),colour="red")+xlab(latex2exp::TeX("$\\sum x$"))+ylab(latex2exp::TeX("$P(\\sum x)$"))

p1+p2+patchwork::plot_annotation(tag_levels = "A")
```

Using this approximation we estimate $R_t$ to be distributed as the ratio of 2 Gamma distributions with the numerator given shape and rate parameters $\alpha'$ and $\beta'$, and the denominator with shape and rate parameters $\alpha''$ and $\beta''$ as described:

$$
\begin{aligned}
R_t &\sim \frac{
  \beta_t'Gamma(\alpha_t',1)
}{
  \beta_t''Gamma(\alpha_t'',1)
}\\
R_t &\sim BetaPrime(\alpha_t',\alpha_t'',1,\frac{\beta_t''}{\beta_t'})\\
\text{Where given: } s \in (1 .. t) &\\
\alpha''_t &= 
\frac{
  \Big(\sum_s \frac{\alpha'_{t-s}\omega_s}{\beta'_{t-s}}\Big)^2
}{
  \sum_s \frac{\alpha'_{t-s}\omega_s^2}{(\beta'_{t-s})^2}
}\\
\beta''_t &= 
\frac{
  \sum_s \frac{\alpha'_{t-s}\omega_s}{\beta'_{t-s}}
}{
  \sum_s \frac{\alpha'_{t-s}\omega_s^2}{(\beta'_{t-s})^2}
}\\
\end{aligned}
$$
This distributional form of $R_t$ as a generalised Beta Prime distribution with three shape parameters ($\alpha_1,\alpha_2,\alpha_3$) and one scale parameter ($\beta$) and with the probability density and cumulative probability functions as given below:

$$
\begin{aligned}
\text{Where } BetaPrime(\alpha_1,\alpha_2,\alpha_3,\beta)&: \\
f(x) &= \frac{
\alpha_3\Big(
  1+\big(
    \frac{x}{\beta}
  \big)^{\alpha_3}
\Big)^{-\alpha_1-\alpha_2}
\Big(
  \frac{x}{\beta}
\Big)^{\alpha_1\alpha_3-1}
}
{
\beta Beta(\alpha_1,\alpha_2)
}\\
F(x) &= I_{
  \frac{x^{\alpha_3}}{x^{\alpha_3}+\beta^{\alpha_3}}
}\big(\alpha_1,\alpha_2\big)
\end{aligned}
$$
and where $I_x$ is the regularised Beta function. This form for the $R_t$ estimate assumes only the Gamma posterior estimate for the Poisson rate, and so can be readily calculated directly from the analytical form of the posteriors identified in the first part of this method. This could be a biased if the denominator of the ratio is not a good estimate for the weighted sum in reality, however the Welch-Satterbach estimator works well when the distributions are not completely different. In our case the fact that the $\lambda_t$ estimates are part of the same time series and scaled by the infectivity profile, does make it quite likely the distributions will have similar characteristics. The method does not account for uncertainty in the infectivity profile $\omega_s$. This can be included by sampling the infectivity profile, and estimating an $R_t$ distribution for each infectivity profile. The estimator for $R_t$ can be made using any posterior estimate of $\lambda_t$, for which we have a range of possibilities based on the desired data window ($\tau$) we wish to use. There are potentially $R_t$ estimates for each of the data window sizes selected for $\lambda_{t,\tau}$ and each of the infectivity profiles. These multiple estimates can be combined either as a mixture distribution, or by sampling as an empirical distribution, from the multiple $R_t$ estimates, to generate an overall summary estimate while retaining uncertainty.

In summary we have presented a method for estimating the expected incidence, the growth rate, and the instantaneous reproduction number from a time series of observed case incidence. This makes a minimal set of assumptions and retains uncertainty throughout the process.


# Growth rate of proportions


$$
\begin{aligned}
I_{t+\tau} &= I_{t}e^{\tau r_t}\\
I_{t-\tau} &= I_{t}e^{-\tau r_t}\\
N_t &= I_t^++I_t^-\\
p_t &= \frac{I_t^+}{N_t} \\
(1-p_t) &= \frac{I_t^-}{N_t} \\
r_t &= r_t^+-r_t^- \\
p_{t-\tau} &= \frac{I_{t}^+e^{-\tau r^+_t}}{I_{t}^+e^{-\tau r^+_t} + I_{t}^-e^{-\tau r^-_t}} \\
p_{t-\tau } &= \frac{N_tp_te^{-\tau r^+_t}}{N_tp_te^{-\tau r^+_t} + N_t(1-p_t)e^{-\tau r^-_t}} \\
p_{t-\tau } &= \frac{p_te^{-\tau r^+_t}}{p_te^{-\tau r^+_t} + (1-p_t)e^{-\tau r^-_t}} \\
p_{t-\tau } &= \frac{p_te^{-\tau r_t}e^{-\tau r^-_t}}{p_te^{-\tau r_t}e^{-\tau r^-_t} + (1-p_t)e^{-\tau r^-_t}} \\
p_{t-\tau } &= \frac{p_te^{-\tau r_t}}{p_te^{-\tau r_t} + (1-p_t)} \\
p_{t-\tau } &= \frac{1}{1 + \frac{(1-p_t)}{p_t}e^{\tau r_t}} \\
\frac{(1-p_t)}{p_t}e^{\tau r_t} &= \frac{1}{p_{t-\tau }} - 1 \\
\Big(\frac{1}{p_t}-1\Big)e^{\tau r_t} &= \frac{1}{p_{t-\tau }} - 1 \\
p'_te^{\tau r_t} & = p'_{t-\tau }\\
r_t & =\frac{1}{\tau}log(\frac{p'_{t-\tau}}{p'_{t}})\\
\end{aligned}
$$

TODO: transform p to p' before doing Bayes?:

$$
\begin{aligned}
P(A \cap B) &= P(A|B)P(B) \\
P(B|A)P(A) &= P(A|B)P(B) \\
\kappa_t &= e^{r_{t}} \\
P(\kappa_{t}|p_{t-\tau},p_{t-\tau+1},\dots,p_{t-1},p_{t}) &= \frac{P(p_{t-\tau},p_{t-\tau+1},\dots,p_{t-1},p_{t}|\kappa_t)P(\kappa_t)}{P(p_{t-\tau},p_{t-\tau+1},\dots,p_{t-1},p_{t})} \\
P(\kappa_{t}|p_{t-\tau},p_{t-\tau+1},\dots,p_{t-1},p_{t}) &\propto P(p_{t-\tau},p_{t-\tau+1},\dots,p_{t-1},p_{t}|\kappa_t)P(\kappa_t) \\
\end{aligned}
$$

$$
\begin{aligned}
P(p_{t-\tau},p_{t-\tau+1},\dots,p_{t-1},p_{t}|\kappa_t) = \prod_0^\tau \frac{1}{1 + \frac{(1-p_t)}{p_t}\kappa_t^{\tau}} \\

\end{aligned}
$$

