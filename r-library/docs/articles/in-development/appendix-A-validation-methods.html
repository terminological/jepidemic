<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Appendix A - Reproduction number validation methodology â€¢ jepidemic</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../../bootstrap-toc.css">
<script src="../../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../pkgdown.js"></script><meta property="og:title" content="Appendix A - Reproduction number validation methodology">
<meta property="og:description" content="jepidemic">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">jepidemic</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.03</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/generating-synthetic-datasets.html">Generating synthetic datasets</a>
    </li>
    <li>
      <a href="../../articles/getting-started.html">Getting started</a>
    </li>
    <li>
      <a href="../../articles/in-development/appendix-A-validation-methods.html">Appendix A - Reproduction number validation methodology</a>
    </li>
    <li>
      <a href="../../articles/in-development/appendix-B-cori-method-jepidemic.html">Appendix B - Renewal equation reproduction number estimation and Jepidemic implementation validation</a>
    </li>
    <li>
      <a href="../../articles/in-development/appendix-C-incidence-and-growth-rate-estimation.html">Appendix C - Incidence and growth rate estimation</a>
    </li>
    <li>
      <a href="../../articles/in-development/appendix-D-bayesian-estimators.html">Appendix D - Bayesian growth rate and reproduction number estimation</a>
    </li>
    <li>
      <a href="../../articles/in-development/bayesian-growth-rate-2.html">Bayesian growth rate estimation</a>
    </li>
    <li>
      <a href="../../articles/in-development/proportion-and-growth-rate-estimation.html">Appendix C - Proportions and growth rate estimation</a>
    </li>
    <li>
      <a href="../../articles/in-development/two-strain-dynamics.html">Two strain dynamics</a>
    </li>
    <li>
      <a href="../../articles/old/cori-method.html">Cori method</a>
    </li>
    <li>
      <a href="../../articles/old/epiestim-testing.html">Testing epiestim</a>
    </li>
    <li>
      <a href="../../articles/old/growth-rate-estimation.html">Growth rate estimation</a>
    </li>
    <li>
      <a href="../../articles/old/growth-rates.html">Cori method</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Appendix A - Reproduction number validation methodology</h1>
                        <h4 class="author">Rob Challen</h4>
            
            <h4 class="date">28-01-2022</h4>
      
      
      <div class="hidden name"><code>appendix-A-validation-methods.Rmd</code></div>

    </div>

    
    
<div id="introduction" class="section level1">
<h1 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h1>
<p>This appendix describes the</p>
<p>The reproduction number represents the ratio between the number of secondary cases resulting from each primary case. At the beginning of an outbreak assuming no prior immunity and a freely mixing population, this is described as the basic reproduction number, <span class="math inline">\(R_0\)</span> <span class="citation">[@vegvariCommentaryUseReproduction]</span>.</p>
<p>The effective reproduction number <span class="math inline">\(R_t\)</span>, is a time varying quantity, which may be defined in terms of the basic reproduction number <span class="math inline">\(R_0\)</span>, the fraction of contacts that people are making at a given time <span class="math inline">\(C_t\)</span>, compared to a freely mixing population, and the fraction of the population that is still susceptible to infection <span class="math inline">\(S_t\)</span>.</p>
<p><span class="math display">\[
R_t = S_t C_t R_0
\]</span> The infectivity profile is another probability distribution. Most often represented in discrete form <span class="math inline">\(\omega_1, \omega_2, \dots, \omega_s\)</span>, that defines the likelihood that a case infected at time <span class="math inline">\(t\)</span> resulted from a case infected between the times <span class="math inline">\(t-s\)</span> and <span class="math inline">\(t-s+1\)</span>. This definition implies that <span class="math inline">\(\omega_{s \leq 0} = 0\)</span> as that would apply to secondary infections resulting from primary infections in the future, and that the discrete time measure <span class="math inline">\(s\)</span> here represents the upper bound of the equivalent continuous unit time interval, rather than, for example, the middle of the interval.</p>
<p>Connecting to the instantaneous reproduction number and the infectivity profile is the quantity <span class="math inline">\(\beta\)</span> which is the "transmissibility" of an infection in an average individual, infected at a given time, <span class="math inline">\(t\)</span>, at a given number of days post infection, <span class="math inline">\(\tau\)</span>. <span class="math inline">\(\beta\)</span> is related to the in host viral load of an infection, and the number of contacts that infected individuals make with susceptible individuals.</p>
<p><span class="math display">\[
\beta_{t,s} = R_t\omega_s
\]</span></p>
<p>There are two basic types of effective reproduction number to consider. The simplest conceptually is the forward-looking definition, in which the reproduction number is the number of secondary infections generated by a single primary infection which occurs at time <span class="math inline">\(t\)</span>, this is known as the case reproduction number <span class="math inline">\(R_t^{case}\)</span> <span class="citation">[@fraserEstimatingIndividualHousehold2007]</span>. The case reproduction number reflects the state of the epidemic at a specific point in time but is limited by the fact that it can only be determined after the event. If we assume <span class="math inline">\(I_0, I_1, \dots, I_t\)</span> is a time series of infection counts, assumed to be drawn from some discrete probability distribution with expected value <span class="math inline">\(\overline{I_t}\)</span> then <span class="math inline">\(R_t^c\)</span> is given by:</p>
<p><span class="math display">\[
\begin{aligned}
R_t^{case} &amp;= \frac{
  \sum_{s=1}^\infty{\overline{I_{t+s}}}\omega_s
}{
  \overline{I_t}
}\\
\end{aligned}
\]</span></p>
<p>We can alternatively define the backward-looking effective reproduction number as the inverse ratio of the number of primary infections that cause the secondary infections observed at time <span class="math inline">\(t\)</span>, this is known as the instantaneous reproduction number, <span class="math inline">\(R_t^{inst}\)</span>. In an evolving epidemic the instantaneous reproduction number is able to be calculated using data that has already been observed and is hence the more useful quantity. The rest of this summary considers the instantaneous version of the effective reproduction number, which we refer to as the reproduction number or <span class="math inline">\(R_t\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
R_t^{inst} &amp;= \frac{
  \overline{I_t}
}{
  \sum_{s=1}^\infty{\overline{I_{t-s}}}\omega_s
}\\
\end{aligned}
\]</span></p>
<p>Alternative methods exist to derive the reproduction number from growth rate <span class="citation">[@wallingaHowGenerationIntervals2007]</span> which we describe in more detail later. These produce a reproduction number that does not have a neat definition in terms of the infection case counts and produces an estimate that is between the two flavours of reproduction number presented here.</p>
<p>The renewal equation method can be used to calculate the instantaneous reproduction number and has a reference implementation in the R package EpiEstim <span class="citation">[@coriEpiEstimEstimateTime2021; @coriNewFrameworkSoftware2013; @thompsonImprovedInferenceTimevarying2019]</span>. This allows for a variety of configuration options to suit different use cases. We concentrate on the estimation of a time series of the reproduction number, for which the main parameters are a single fixed mean and standard deviation for the <span class="math inline">\(R_t\)</span> prior, the window over which the estimate will be performed, and an infectivity profile. The infectivity profile can be specified in a number of ways, but for our purposes we concentrate on the discrete empirical version ("non_parametric_si" option). Figure A1 shows the behaviour of EpiEstim on an outbreak in the Flu2009 data set included in EpiEstim for a range of different estimation windows, and using the given infectivity profile.</p>
<pre><code>#&gt; Error: Aesthetics must be valid data columns. Problematic aesthetic(s): x = date. 
#&gt; Did you mistype the name of a data column or forget to add after_stat()?</code></pre>
<p><em>Figure A1: Panel A shows a case count for the Flu2009 outbreak data set, Panel B shows the discrete infectivity profile, and panel C shows the <span class="math inline">\(R_t\)</span> time series estimates for a range of different estimation windows. Vertical error bars show the confidence in <span class="math inline">\(R_t\)</span> whereas horizontal error bars show the date range for which the assumption of constant <span class="math inline">\(R_t\)</span> for each estimate is applied</em></p>
<p>This simple example demonstrates some variability in the estimates that we wished to be able to quantify, in that there is a clear trade-off between bias and variance in the window selection. This is determined in part by the case counts within the time window of the estimate. With smaller estimate windows and less data at either end of the time series the estimates revert to the prior <span class="math inline">\(R_t\)</span> distribution, whereas with longer windows, at least in this scale and duration of outbreak, the detail is lost. There is a need to be able to assess the performance of an individual estimation method and parameterisation, against a standard and to be able to compare against each other. The purpose of this paper is to describe the validation procedure and define the associated quality metrics we employ to compare estimates. This will be described in terms of <span class="math inline">\(R_t\)</span> estimation using three parameterisations of EpiEstim (14 day window, 7 day window and 4 day window) but is extensible to comparisons between other methods and to other observations we may wish to estimate such as case incidence and exponential growth rates as well.</p>
</div>
<div id="validation-methodology" class="section level1">
<h1 class="hasAnchor">
<a href="#validation-methodology" class="anchor"></a>Validation methodology</h1>
<p>We construct synthetic data sets with known values of expected incidence, <span class="math inline">\(R_t\)</span> and exponential growth rate. This synthetic data set is generated using an initial case count, and a time series of exponential growth rate values. The growth rate time series are either a step function with 6 different predefined levels or a smooth cubic spline passing through 6 predefined control points, over the course of a theoretical year. The growth rate time series is accumulated and applied to the initial incidence to generate a time series of expected incidence. Theoretical values of <span class="math inline">\(R_t\)</span> are calculated from the growth rate using the methods of Wallinga et al (2007) <span class="citation">[@wallingaHowGenerationIntervals2007]</span> and assuming a synthetic generation time which is gamma distributed with mean (<span class="math inline">\(\mu\)</span>) of 5 days and standard deviation (<span class="math inline">\(\sigma\)</span>) of 4. This uses the following relationship, where <span class="math inline">\(M\)</span> is the moment generating function of the gamma distribution with shape parameter <span class="math inline">\(\alpha = \mu^2/\sigma^2\)</span> and rate parameter <span class="math inline">\(\beta = \mu/\sigma^2\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
R_t &amp;= \frac{1}{M(-r_t)} \\
&amp;= \Big(1-\frac{r_t}{\beta}\Big)^{-\alpha} \\
&amp;= \Big(1-\frac{r_t\sigma^2}{\mu}\Big)^{-\mu^2/\sigma^2}
\end{aligned}
\]</span></p>
<p>From each simulation of case incidence random bootstrap samples are drawn from a Poisson distribution whose rate is the expected incidence. Optionally a weekend effect is simulated, a second Poisson sample is made on Saturday, Sunday and Mondays in the time series with a rate given as a fraction of the expected incidence (0%, 3% or 10%). This second random sample is subtracted from the first on Saturdays and Sundays, and added on Mondays, giving a weekly cycle to case counts similar to that seen in reality. Any resulting negative values are set to zero.</p>
<p>We simulate using one smooth and one stepped growth rate time series in combination with three levels of weekend effect mentioned above, and with two initial case counts (100 and 10,000), giving an overall 12 different configurations. The synthetic gamma distributed generation time is discretised on whole day intervals as required by the estimation methods, imposing that the generation time is known precisely and does not vary for this validation analysis.</p>
<p><img src="../../../../../../../home/terminological/Dropbox/sarscov2/r-estimation-methodology/epiEstimSynthetic14day-2022-01-28.png" width="885"></p>
<p><em>Figure A2: Panel A shows a case count, for a synthetic data sets generated from a spline function for growth rate, with low (100) initial case counts, and a 10% weekly variability. Expected incidence curves are shown in red and black points represent one set of data samples (out of 10 generated). Panel B shows the estimated reproduction number using the renewal equation method with a gamma distributed infectivity profile (mean 5 days, sd 4), and a fixed window of 14 days. The red line shows the theoretical <span class="math inline">\(R_t\)</span> value.</em></p>
<p>In Figure A2 we show 1 sample from the smooth test configuration, with 10% weekly variation and 100 initial cases. The reproduction number is estimated with EpiEstim in its default configuration with a fixed window of 14 days. Estimate uncertainty is larger when case numbers are low. The estimates (black lines in panel B) are clearly lagged compared to the theoretical <span class="math inline">\(R_t\)</span> value, particularly noticeably when case counts are high. At the beginning of the time series the estimate is consistently inappropriately high reflecting the prior distribution of the <span class="math inline">\(R_t\)</span> estimate, and a boundary effect at the beginning of the time series. Although only one time series from each configuration are being shown here, 10 are generated, to simulate data variability. Similarly we are showing two simulation configurations here as examples, but overall we have 12 configurations which describe different combinations of smoothness, weekend effect, and initial incidence.</p>
</div>
<div id="quantifying-estimate-delay" class="section level1">
<h1 class="hasAnchor">
<a href="#quantifying-estimate-delay" class="anchor"></a>Quantifying estimate delay</h1>
<p>Before we can assess the quality of the estimates we need to determine how much delay there is in the estimates. To do this we use another synthetic data set which is generated from a triangular wave for growth rate with period of 91 days. For a range of different time lags (0 days to 28 days) we calculate the root mean squared error between the theoretical <span class="math inline">\(R_t\)</span> and median of the estimate shifted backwards in time. In Figure A3 panel B we quantify the delay between the theoretical and the estimated <span class="math inline">\(R_t\)</span>, for a range of estimation methods. The delay is the result of the combination of estimate being a backward looking instantaneous reproduction number, which integrates information from the past according to the generation time distribution, and the method's estimation window, which in this case is 4, 7, or 14 days. From visual inspection of the time series in panel A there is no compelling evidence that the lag is significantly different from one time period to another, although there is a hint that the delay is shorter when case numbers are high and rising. For subsequent analysis each model's estimates of <span class="math inline">\(R_t\)</span> are adjusted backwards by the nearest integer number of days derived from this lag analysis. It must be pointed out that the true value of the reproduction number is calculated using the methods of Wallinga et al (2007) which produces a different type of reproduction number to the instantaneous reproduction number produced by the renewal equation methods presented here.</p>
<p><img src="../../../../../../../home/terminological/Dropbox/sarscov2/r-estimation-methodology/lagAnalysis-2022-01-28.png" width="885"></p>
<p><em>Figure A3: Analysis of <span class="math inline">\(R_t\)</span> estimate time delays. In panel A as estimate of <span class="math inline">\(R_t\)</span> based on a periodic growth rate clearly shows estimate lag in all methods of estimating <span class="math inline">\(R_t\)</span>. In panel B the root mean squared error for a range of values of lag are calculated. The minimum RMSE is depends on the estimation model employed, however there is not an obvious linear relationship.</em></p>
</div>
<div id="quantification-of-accuracy-bias-and-calibration" class="section level1">
<h1 class="hasAnchor">
<a href="#quantification-of-accuracy-bias-and-calibration" class="anchor"></a>Quantification of accuracy, bias and calibration</h1>
<p>To investigate the bias and calibration of the estimation method we compare four statistics derived from the <span class="math inline">\(R_t\)</span> estimates and associated theoretical <span class="math inline">\(R_t\)</span> values, using the lag adjusted estimates. Estimates of <span class="math inline">\(R_t\)</span> include a mean, standard deviation and 2.5%, 5%, 25%, 50%, 75%, 95% and 97.5% quantiles. To identify bias, for each point we calculate the difference between median estimate and theoretical <span class="math inline">\(R_t\)</span> values as the residual (<span class="math inline">\(\xi_{bias}\)</span>). On this measure a value of 0 represents an unbiased estimate, and bias is presented on the same scale as the <span class="math inline">\(R_t\)</span> estimate.</p>
<p>To assess precision we measure the calibration of the estimate (<span class="math inline">\(\xi_{cal}\)</span>). This is defined as 1 if the actual value is within the confidence intervals of the estimate, and 0 if the actual value is outside. A well calibrated set of estimates should have an average close to 1, and poorly calibrated estimates will be closer to zero.</p>
<p>We examine the confidence of the estimate using a continuous data analogy to the verification rank histogram <span class="citation">[@andersonMethodProducingEvaluating1996; @brockerReliabilityAnalysisMulticategorical2008; @siegertSpecsVerificationForecastVerification2020]</span>. The verification rank histogram examines the distribution of the ranks of each of the true values among the collection of associated point estimates. The shape of the distribution of these ranks determines the appropriateness of the confidence limits <span class="citation">[@hamillInterpretationRankHistograms2001]</span>. Appropriate variation in point estimates lead to a flat histogram with the true values falling evenly over the point estimates. Not enough variability in the point estimates results in a U-shaped histogram, too much in a dome shaped histogram. For our purposes we have individual estimates that describe the estimate as a a posterior distribution through a set of quantiles. This estimate is associated with a single true <span class="math inline">\(R_t\)</span> value. We can derive a cumulative density function of each estimate (<span class="math inline">\(F(x)\)</span>) from the quantiles, by filling a monotonic Hermite spline to them, and use this CDF to estimate the quantile of the theoretical <span class="math inline">\(R_t\)</span> value with respect to the estimate distribution [<span class="math inline">\(F(x = R_t)\)</span>]. The quantile of the actual <span class="math inline">\(R_t\)</span> value (<span class="math inline">\(\xi_{quant}\)</span>) with regards to the posterior distribution of the <span class="math inline">\(R_t\)</span> estimate is equivalent to the rank in the rank histogram, and we define this as the "actual value quantile" for all the paired estimate distributions and true <span class="math inline">\(R_t\)</span> values. The density plot of the actual value quantile, for a range of estimates, we define as the quantile density plot and thsi has the same interpretation as the rank histogram.</p>
<p>The continuous ranked probability score (CRPS) is a measure of performance for probabilistic estimates of a scalar observation. It is a quadratic measure of the difference between the estimate cumulative distribution function (CDF) and the empirical CDF of the observation <span class="citation">[@zamoEstimationContinuousRanked2018]</span>. Loosely speaking, it describes the mass of the probability density function that is closer to the mean of the estimate, than the true value is. For each estimate it can be calculated directly from the paired CDF of an estimate (<span class="math inline">\(F(x)\)</span>) and a scalar true value (<span class="math inline">\(y\)</span>) as the following, where <span class="math inline">\(\mathbb{I}\)</span> is the indicator function. This estimate-by-estimate value is referred to as the instantaneous CRPS (<span class="math inline">\(\xi_{cprs}\)</span>), and low values imply higher quality estimates:</p>
<p><span class="math display">\[
\begin{aligned}
CRPS_{inst}(F,y) &amp;= \int_{-\infty}^\infty \Big(F(x) - \mathbb{I}(x \ge y)\Big)^2dx\\
\end{aligned}
\]</span></p>
<p>In summary empirical distributions for the 4 metrics are derived from each individual estimate:</p>
<p><span class="math display">\[
\begin{aligned}
\text{bias}: \xi_{bias} &amp;= \Big[F_n^{-1}(0.5) - R_{t,n}\Big]\\
\text{calibration}: \xi_{cal} &amp;= \Big[\mathbb{I}\Big(F_n^{-1}(0.025) \le R_{t,n} \le F_n^{-1}(0.975)\Big)\Big]\\
\text{actual value quantile}: \xi_{quant} &amp;= \Big[F_n(x=R_{t,n})\Big]\\
\text{instantaneous continuous rank probability score}: \xi_{cprs} &amp;= \Big[CPRS_{inst}(F_n,R_{t,n})\Big]\\
\end{aligned}
\]</span> In our case, of particular interest in assessing estimates of <span class="math inline">\(R_t\)</span> is the critical threshold of <span class="math inline">\(R_t=1\)</span> when an epidemic transitions from growth to decline or vice-versa. A specific measure for this scenario detects if the confidence limits of an estimate are both the opposite side of 1 to the true value. An ideal situation is that this never happens. This "critical threshold" metric (<span class="math inline">\(\xi_{crit}\)</span>) is defined as follows assuming the sign function is defined as <span class="math inline">\(sgn(x) = x/|x|\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\text{critical threshold}: \xi_{crit} &amp;= \Big[\mathbb{I}\Big(sgn(F_n^{-1}(0.025)-1) = sgn(R_{t,n}-1) || sgn(R_{t,n}-1) = sgn(F_n^{-1}(0.975)-1)\Big)\Big]\\
\end{aligned}
\]</span></p>
<p>When we wish to compare the performance of a given method against another we need aggregate the estimate level scores achieved against a particular validation data set. The median and inter-quartile ranges of the bias and instantaneous continuous rank probability score of all estimates can be simply calculated to give an overall metric for the performance of the estimation method. Likewise the calibration and critical threshold of each estimate can be aggregated into a single percentage for the method.</p>
<p>The actual value quantiles (<span class="math inline">\(\xi_{quant}\)</span>), are inspected visually and have the same interpretation as the verification rank histogram, with a U-shaped density curve representing over-precision and a dome shaped density curve representing excessive uncertainty, and an ideal result being a uniform distribution. The average value of <span class="math inline">\(\xi_{quant}\)</span> should be close to 0.5. To quantify the shape of the quantile density plot, define the deviation of the values <span class="math inline">\(\xi_{quant}\)</span> for each of <span class="math inline">\(N\)</span> estimates from the expected value of 0.5, and compare this to the ideal value, which is the standard deviation of a uniform distribution with support between 0 and 1 (<span class="math inline">\(\sqrt{\frac{1}{12}}\)</span> = 0.289). This gives us the following "quantile deviation" metric, which ranges between 0.211 and -0.289 with positive values representing more deviation than expected, a U-shaped quantile density plot and over precise estimation methods, and negative values representing less deviation than expected, a dome shaped quantile density plot and excessive uncertainty in the estimation method:</p>
<p><span class="math display">\[
\text{quantile deviation}: Q_{dev} = \sqrt{\frac{\sum(\xi_{quant}-0.5)^2}{N-1}} - \sqrt{\frac{1}{12}}
\]</span></p>
<p>These metrics can be used to summarise the performance of each individual estimation method and compare it to others. At a top level we can aggregate the individual estimates over time and over the different simulations, to produce a single set of 5 metrics for each estimation method, with a focus on the overall performance. The quality metrics are visualised as box plots for the bias and CRPS, a simple proportion for the calibration, and critical threshold measure, and the distribution shape for the quantile density in the form of a violin plot. In this case the U shaped verification rank histogram becomes an I shaped quantile density violin plot, representing over-precision, and a dome shaped verification rank histogram, becomes an O shaped quantile density violin plot, representing excessive uncertainty.</p>
<p><img src="../../../../../../../home/terminological/Dropbox/sarscov2/r-estimation-methodology/errorSummaryComparison-2022-01-28.png" width="885"></p>
<p><em>Figure A4: Estimate quality metric summaries for multiple estimation methods. In this instance we compare the performance of the renewal equation method with 14, 7, and 4 days as the windowing. Panel A show summary statistics for the bias of <span class="math inline">\(R_t\)</span> estimates, panel B shows the calibration. Panel C shows the quantile density, panel D the CRPS and panel E the critical threshold calibration.</em></p>
<p>The top level comparison in Figure A4 shows similar performance of the different methods on most metrics. Unsurprisingly there is more spread in the error of estimates using a 4 day window seen in Panel A. All methods are similarly calibrated (panel B - higher is better) and show similar levels of over precision (Panel C). The CPRS (lower is better) is clearly higher for the 7 day and 4 day estimates, with the 14 day estimate performing best overall, reflecting the decrease in noise seen in panel A. 14 day model is also best at predicting growth when the epidemic is in decline and vice-versa, with less that 1% critical threshold error, although this is relatively good across all methods. The visual comparison is backed up by the quantitative metrics presented in table A1, which also gives us a quantitative comparison for the quantile divergence which confirms the over precision is worst (highest quantile deviation) for the 14 day estimates.</p>
<p><em>Table A1: Estimate quality metric summaries for the three estimation methods compared here.</em> <img src="../../../../../../../home/terminological/Dropbox/sarscov2/r-estimation-methodology/errorSummaryComparisonTable-2022-01-28.png" width="885"></p>
<p>To further assess the details of why the individual methods differ we investigate an intermediate level of detail, in which the metrics are broken down by differences in the simulated data as described below.</p>
<p><img src="../../../../../../../home/terminological/Dropbox/sarscov2/r-estimation-methodology/errorSummaryAnalysis-2022-01-28.png" width="885"></p>
<p><em>Figure A5: Estimate quality metric intermediate detail and model comparison. Panel A-D show summary statistics for the bias of <span class="math inline">\(R_t\)</span> estimates broken down by the simulation smoothness, weekly variability, initial incidence, and time series boundary status. Panels E-F shows the calibration for the same subdivisions. Panels I-L shows the quantile density, panel M-P the CRPS, and panel Q-T the critical thresholds for the same subdivisions.</em></p>
<p>At this intermediate detail the aggregation of the quality metrics are faceted by the salient features of the simulation, which inform us of where the estimators strengths and weaknesses are. These including the the smoothness of the growth rate function (first column), the degree of weekend effect (second column), initial size (third column), and also the proximity of the estimate to either end of the time series (final column). This last measure is defined as whether the estimate is within 21 days of the start or end of the time series. The latter is important because boundary effects particularly on most recent end of the time series may be important for real time estimates of <span class="math inline">\(R_t\)</span>. This intermediate view of the quality of the estimates expands on the findings from figure A4, for example, in Panel G and K in Figure A5, we note the low calibration and excessive precision of all the estimates is due to the high incidence simulations. In panel O however we note that the CPRS is lower (better) in the higher incidence scenarios suggesting that the over-precise estimates do come with improved accuracy (also seen in panel C).</p>
<p>The boundary effect analysis in panels D,H,L,P demonstrates that all the methods investigated here are biased high in the first 3 weeks, and poorly calibrated. The critical threshold metric (panels Q,R,S &amp; T) shows that the 14 day estimate method is able to detecting transitions between growth to decline well. The simulations with high weekly variation is seen to the accuracy of the 4 day estimate on this measure (panel R). The impact of the weekly variation on the 4 day estimate is also seen in panel B where increase spread of error occurs with increasing variation.</p>
<p>On the face of this comparison there is evidence to favour the 14 day estimate methodology.</p>
<p>To further understand the performance of the 14 day model, we can also visualize these metrics over time and between selected simulation configurations. For the single estimation methodology (14 days window) we can compare low and high incidence and smoothness of the simulation. We present in detail the 4 simulations with no weekend effects, 2 of which are based on smoothly varying growth rates, and 2 on stepped growth rates, with different numbers of initial cases (low incidence: 100 and high incidence: 10000).</p>
<pre><code>#&gt; Error in loadNamespace(name): there is no package called 'ukcovidtools'</code></pre>
<p><img src="../../../../../../../home/terminological/Dropbox/sarscov2/r-estimation-methodology/errorAnalysis-2022-01-28.png" width="885"></p>
<p><em>Figure A6: Time variations in estimate quality metrics for simulations with no weekly variability. Panel A shows the modelled case count, and one sample from the simulation for 4 synthetic data simulation configurations. Panel B shows the simulation reproduction number (red) and one estimate (black) based on one sample from the simulation, using the renewal equation method with a gamma distributed infectivity profile (mean 5 days, sd 4), and a fixed window of 14 days with the time corrected to account for estimate lags. Panel C shows the bias of each individual <span class="math inline">\(R_t\)</span> estimate and rolling quantiles over a 28 days period. Panel D shows the calibration of individual estimates and the rolling mean of the calibration (14 day window). Panel E shows the quantile density over time (with density as the shade), the rolling 10%, 30% 50% 70% and 90% quantiles are shown as grey lines calculated over a 28 day window. In panel F the instantaneous CRPS for individual estimates and the rolling quantiles are shown. In Panel G the critical threshold measure shows where the estimates confidence limits are the other side of the <span class="math inline">\(R_t\)</span> critical threshold of 1 to the actual value. In all panels quantiles shown in red are 2.5% and 97.5% if dotted, 25% and 75% if dashed and the median is a solid red line. Blue horizontal lines represent the median for the statistic for the whole time series.</em></p>
<p>For this selection of simulation configurations and estimation method we again find that there is no evidence of strong systematic bias. Over all simulations in Figure A6 the bias is 0.0029 [IQR -0.0078 â€“ 0.016] (reproduction number is unit-less). These is a suggestion in Panel C that this varies over time. The calibration of the estimates is very variable, and overall 50.6% [95% CI 50.1% â€“ 51.1%] of the estimates include the actual value in their confidence intervals. Panel D demonstrates that this is far worse when case numbers are high and during step changes in the reproduction number, which is due to over precision in the estimates. The quantile density (panel E) backs this up with the distribution swinging from one extreme to the other in all the parts of the simulations with high incidence, implying an over-precise estimate. However although the calibration is poor and the quantile density is far from a flat distribution, in the higher incidence simulations the bias of the estimates is smaller. When these observations are taken together, despite the over precision, we see in panel E that the CRPS is in fact lowest for the higher incidence simulations. Overall we can conclude that that this estimation methodology is most accurate with high case numbers but produces slightly biased estimates with excessive confidence. It performs less well when there is an abrupt change in <span class="math inline">\(R_t\)</span> (panel G), and this means that 0.6% [95% CI 0.6% â€“ 0.7%] of the estimates using this method incorrectly predict growth when in fact the epidemic is in decline, or vice-versa.</p>
</div>
<div id="summary" class="section level1">
<h1 class="hasAnchor">
<a href="#summary" class="anchor"></a>Summary</h1>
<p>This appendix describes a methodology for verifying the estimation of <span class="math inline">\(R_t\)</span> using synthetic data sets designed to highlight particular issues, by comparing the posterior distributions of estimates to the known <span class="math inline">\(R_t\)</span> values used to generate the data sets. The different methods compared here have different degrees of delay between the actual <span class="math inline">\(R_t\)</span> using in creating the data set and the estimate ranging from 7.75 to 10.85 days. The performance of the estimation methods can be summarized with 5 metrics, the bias, calibration, quantile deviation, continuous ranked probability score, and the critical threshold measure. At a summary level the combination of these metrics allows performance of different estimate methods to be quantitatively compared, and demonstrate that despite a degree of over precision, and resulting lower calibration, the 14 day estimator performs best overall. Furthermore by comparing performance of different estimation methods against different simulations with specific features, we can qualitatively assess situations in which individual estimation methodologies perform better or worse. A detailed analysis of the time series of the metrics gives us further insight into situations where each given estimate method may under-perform.</p>
<p>This validation methodology is extensible to other metrics other than <span class="math inline">\(R_t\)</span> where a set of confidence intervals or quantiles are available, particularly estimates of growth rate and incidence.</p>
</div>
<div id="references" class="section level1">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<div id="refs">

</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Rob Challen, terminological.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
