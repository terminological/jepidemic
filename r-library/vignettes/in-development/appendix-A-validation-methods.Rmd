---
title: "Appendix A - Reproduction number validation methodology"
author: "Rob Challen"
date: '`r format(Sys.Date(), "%d-%m-%Y")`'
# output: 
#   pdf_document :
#     fig_caption: yes
#     keep_tex: TRUE
# header-includes:
#  \usepackage{float}
#  \usepackage{mathtools}
#  \usepackage{amsmath}
#  \floatplacement{figure}{H}    
#  \DeclareRobustCommand{\[}{\begin{equation*}}
#  \DeclareRobustCommand{\]}{\end{equation*}}
#  \newcounter{tagno}
#  \setcounter{tagno}{0}
#  \let\amsmathtag\tag
#  \renewcommand{\tag}[1]{\amsmathtag{\thetagno} \label{#1} \stepcounter{tagno}}
# knit: (function(inputFile, encoding,...) {
#   rmarkdown::render(inputFile, encoding = encoding, output_dir = "~/Dropbox/sarscov2/r-estimation-methodology", output_file=paste0('appendix-A-validation-methods-',Sys.Date(),'.pdf')) })
# fig_width: 7
# fig_height: 5
# out.width: "100%"
# bibliography: jepidemic.bib
# csl: jepidemic.csl
# vignette: >
#   %\VignetteIndexEntry{Reproduction number validation methodology}
#   %\VignetteEngine{knitr::rmarkdown}
#   %\VignetteEncoding{UTF-8}
output: 
  latex_fragment
knit: (function(inputFile, encoding,...) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "~/Dropbox/sarscov2/r-estimation-methodology", output_file=paste0('appendix-A-validation-methods-',Sys.Date(),'.tex')) })
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  error = TRUE
)

here::i_am("vignettes/in-development/appendix-A-validation-methods.Rmd")
source(here::here("vignettes/common-setup.R"))
library(EpiEstim)
```

# Introduction

This appendix describes the 

The reproduction number represents the ratio between the number of secondary cases resulting from each primary case. At the beginning of an outbreak assuming no prior immunity and a freely mixing population, this is described as the basic reproduction number, $R_0$ [@vegvariCommentaryUseReproduction].

The effective reproduction number $R_t$, is a time varying quantity, which may be defined in terms of the basic reproduction number $R_0$, the fraction of contacts that people are making at a given time $C_t$, compared to a freely mixing population, and the fraction of the population that is still susceptible to infection $S_t$.

$$
R_t = S_t C_t R_0
$$
The infectivity profile is another probability distribution. Most often represented in discrete form $\omega_1, \omega_2, \dots, \omega_s$, that defines the likelihood that a case infected at time $t$ resulted from a case infected between the times $t-s$ and $t-s+1$. This definition implies that $\omega_{s \leq 0} = 0$ as that would apply to secondary infections resulting from primary infections in the future, and that the discrete time measure $s$ here represents the upper bound of the equivalent continuous unit time interval, rather than, for example, the middle of the interval.

Connecting to the instantaneous reproduction number and the infectivity profile is the quantity $\beta$ which is the "transmissibility" of an infection in an average individual, infected at a given time, $t$, at a given number of days post infection, $\tau$. $\beta$ is related to the in host viral load of an infection, and the number of contacts that infected individuals make with susceptible individuals.

$$
\beta_{t,s} = R_t\omega_s
$$

There are two basic types of effective reproduction number to consider. The simplest conceptually is the forward-looking definition, in which the reproduction number is the number of secondary infections generated by a single primary infection which occurs at time $t$, this is known as the case reproduction number $R_t^{case}$ [@fraserEstimatingIndividualHousehold2007]. The case reproduction number reflects the state of the epidemic at a specific point in time but is limited by the fact that it can only be determined after the event. If we assume $I_0, I_1, \dots, I_t$ is a time series of infection counts, assumed to be drawn from some discrete probability distribution with expected value $\overline{I_t}$ then $R_t^c$ is given by: 

$$
\begin{aligned}
R_t^{case} &= \frac{
  \sum_{s=1}^\infty{\overline{I_{t+s}}}\omega_s
}{
  \overline{I_t}
}\\
\end{aligned}
$$
 
We can alternatively define the backward-looking effective reproduction number as the inverse ratio of the number of primary infections that cause the secondary infections observed at time $t$, this is known as the instantaneous reproduction number, $R_t^{inst}$. In an evolving epidemic the instantaneous reproduction number is able to be calculated using data that has already been observed and is hence the more useful quantity. The rest of this summary considers the instantaneous version of the effective reproduction number, which we refer to as the reproduction number or $R_t$.

$$
\begin{aligned}
R_t^{inst} &= \frac{
  \overline{I_t}
}{
  \sum_{s=1}^\infty{\overline{I_{t-s}}}\omega_s
}\\
\end{aligned}
$$

Alternative methods exist to derive the reproduction number from growth rate [@wallingaHowGenerationIntervals2007] which we describe in more detail later. These produce a reproduction number that does not have a neat definition in terms of the infection case counts and produces an estimate that is between the two flavours of reproduction number presented here.

The renewal equation method can be used to calculate the instantaneous reproduction number and has a reference implementation in the R package EpiEstim [@coriEpiEstimEstimateTime2021; @coriNewFrameworkSoftware2013; @thompsonImprovedInferenceTimevarying2019]. This allows for a variety of configuration options to suit different use cases. We concentrate on the estimation of a time series of the reproduction number, for which the main parameters are a single fixed mean and standard deviation for the $R_t$ prior, the window over which the estimate will be performed, and an infectivity profile. The infectivity profile can be specified in a number of ways, but for our purposes we concentrate on the discrete empirical version ("non_parametric_si" option). Figure A1 shows the behaviour of EpiEstim on an outbreak in the Flu2009 data set included in EpiEstim for a range of different estimation windows, and using the given infectivity profile.

```{r}
epiest = bind_rows(lapply(1:15, function(window) {
  tau = window-1
  tmp = EpiEstim::estimate_R(Flu2009$incidence, 
                             config = make_config(method = "non_parametric_si",
                                                  si_distr = Flu2009$si_distr, 
                                                  mean_prior=5, std_prior=4,
                                                  t_start=2:(32-tau),t_end=(2+tau):32))
  return(tmp$R %>% mutate(Rt.StartDate = tmp$date[t_start],
                          Rt.EndDate = tmp$date[t_end],
                          Rt.Mean = `Mean(R)`, 
                          Rt.Quantile.0.5 = `Median(R)`, 
                          Rt.Quantile.0.025 = `Quantile.0.025(R)`, 
                          Rt.Quantile.0.975 = `Quantile.0.975(R)`,
                          Rt.Window = window
  ))
}))
p1 = ggplot(Flu2009$incidence, aes(x=date,y=I))+
    geom_point(size=0.5)+
    geom_line(alpha=0.1)+
    ylab("cases")+xlab(NULL)


p2 = ggplot(tibble(y=Flu2009$si_distr) %>% mutate(x1=row_number()-1,x0=x1-1), aes(x=(x1+x0)/2,y=y))+
  geom_bar(stat="identity", width=0.9, colour="black",fill=NA)+
  scale_x_continuous(breaks=-1:14)+
  xlab("days since primary infection")+
  ylab("P(secondary infection on given day|secondary infection)")

p3 = ggplot(epiest %>% filter(Rt.Window<=10) %>% filter(!is.nan(Rt.Mean)),aes(x=Rt.EndDate, y=Rt.Mean, colour=as.factor(Rt.Window)))+
    geom_point(size=0.5) +
    geom_errorbarh(aes(x=Rt.EndDate, xmin=Rt.StartDate, xmax=Rt.EndDate), alpha=0.3) +
    geom_errorbar(aes(y=Rt.Quantile.0.5, ymin = Rt.Quantile.0.025, ymax = Rt.Quantile.0.975), alpha=0.3)+
    guides(colour="none")+coord_cartesian(ylim=c(0,5))+
    geom_hline(yintercept=1,colour="grey50",inherit.aes=FALSE)+
    xlab("date")+
    ylab(latex2exp::TeX("$R_t$"))+facet_wrap(vars(Rt.Window),ncol=5)+standardPrintOutput::narrowAndTall()+xlab(NULL)

  

design = "
111122
333322
333322
"
p4 = p1+p2+p3+plot_annotation(tag_levels = "A")+plot_layout(design = design)
p4 %>% saveHalfPageFigure(output("epiEstimFlu2009"))
```

*Figure A1: Panel A shows a case count for the Flu2009 outbreak data set, Panel B shows the discrete infectivity profile, and panel C shows the $R_t$ time series estimates for a range of different estimation windows. Vertical error bars show the confidence in $R_t$ whereas horizontal error bars show the date range for which the assumption of constant $R_t$ for each estimate is applied*

This simple example demonstrates some variability in the estimates that we wished to be able to quantify, in that there is a clear trade-off between bias and variance in the window selection. This is determined in part by the case counts within the time window of the estimate. With smaller estimate windows and less data at either end of the time series the estimates revert to the prior $R_t$ distribution, whereas with longer windows, at least in this scale and duration of outbreak, the detail is lost. There is a need to be able to assess the performance of an individual estimation method and parameterisation, against a standard and to be able to compare against each other. The purpose of this paper is to describe the validation procedure and define the associated quality metrics we employ to compare estimates. This will be described in terms of $R_t$ estimation using three parameterisations of EpiEstim (14 day window, 7 day window and 4 day window) but is extensible to comparisons between other methods and to other observations we may wish to estimate such as case incidence and exponential growth rates as well.

# Validation methodology

We construct synthetic data sets with known values of expected incidence, $R_t$ and exponential growth rate. This synthetic data set is generated using an initial case count, and a time series of exponential growth rate values. The growth rate time series are either a step function with 6 different predefined levels or a smooth cubic spline passing through 6 predefined control points, over the course of a theoretical year. The growth rate time series is accumulated and applied to the initial incidence to generate a time series of expected incidence. Theoretical values of $R_t$ are calculated from the growth rate using the methods of Wallinga et al (2007) [@wallingaHowGenerationIntervals2007] and assuming a synthetic generation time which is gamma distributed with mean ($\mu$) of 5 days and standard deviation ($\sigma$) of 4. This uses the following relationship, where $M$ is the moment generating function of the gamma distribution with shape parameter $\alpha = \mu^2/\sigma^2$ and rate parameter $\beta = \mu/\sigma^2$.

$$
\begin{aligned}
R_t &= \frac{1}{M(-r_t)} \\
&= \Big(1-\frac{r_t}{\beta}\Big)^{-\alpha} \\
&= \Big(1-\frac{r_t\sigma^2}{\mu}\Big)^{-\mu^2/\sigma^2}
\end{aligned}
$$

From each simulation of case incidence random bootstrap samples are drawn from a Poisson distribution whose rate is the expected incidence. Optionally a weekend effect is simulated, a second Poisson sample is made on Saturday, Sunday and Mondays in the time series with a rate given as a fraction of the expected incidence (0%, 3% or 10%). This second random sample is subtracted from the first on Saturdays and Sundays, and added on Mondays, giving a weekly cycle to case counts similar to that seen in reality. Any resulting negative values are set to zero. 

We simulate using one smooth and one stepped growth rate time series in combination with three levels of weekend effect mentioned above, and with two initial case counts (100 and 10,000), giving an overall 12 different configurations. The synthetic gamma distributed generation time is discretised on whole day intervals as required by the estimation methods, imposing that the generation time is known precisely and does not vary for this validation analysis.


```{r}

rtEstimator = function(window) {
  return(function(ts, infectivityProfile) {
    # configure epi-estim 
    epiestimRtEstimate(ts, infectivityProfile$yMatrix, window = window)
  })
}

# each estimator function must take a single time series and a single infectivity_profile object and produce a single 
# time series of the estimates. The input time-series will have a "date" and a "value" column.

estimators = tibble(
  model = forcats::as_factor(c(
    "EpiEstim: 4 day",
    "EpiEstim: 7 day",
    "EpiEstim: 14 day"
  )), 
  estimFn = c(
    rtEstimator(4),
    rtEstimator(7),
    rtEstimator(14)
  )
)

lagAnalysisResult = lagAnalysis(estimators)
qualityEstimates = syntheticEstimates(estimators)
qualityAnalysisResult = validationMetrics(qualityEstimates, lagAnalysisResult$modelLag, estimation="Rt")


p3 = qualityAnalysisResult %>% estimationExamplePlot(bootstraps = 2)
p3 %>% saveHalfPageFigure(output("epiEstimSynthetic14day"))

```

*Figure A2: Panel A shows a case count, for a synthetic data sets generated from a spline function for growth rate, with low (100) initial case counts, and a 10% weekly variability. Expected incidence curves are shown in red and black points represent one set of data samples (out of 10 generated). Panel B shows the estimated reproduction number using the renewal equation method with a gamma distributed infectivity profile (mean 5 days, sd 4), and a fixed window of 14 days. The red line shows the theoretical $R_t$ value.*

In Figure A2 we show 1 sample from the smooth test configuration, with 10% weekly variation and 100 initial cases. The reproduction number is estimated with EpiEstim in its default configuration with a fixed window of 14 days. Estimate uncertainty is larger when case numbers are low. The estimates (black lines in panel B) are clearly lagged compared to the theoretical $R_t$ value, particularly noticeably when case counts are high. At the beginning of the time series the estimate is consistently inappropriately high reflecting the prior distribution of the $R_t$ estimate, and a boundary effect at the beginning of the time series. Although only one time series from each configuration are being shown here, 10 are generated, to simulate data variability. Similarly we are showing two simulation configurations here as examples, but overall we have 12 configurations which describe different combinations of smoothness, weekend effect, and initial incidence.

# Quantifying estimate delay

Before we can assess the quality of the estimates we need to determine how much delay there is in the estimates. To do this we use another synthetic data set which is generated from a triangular wave for growth rate with period of 91 days. For a range of different time lags (0 days to 28 days) we calculate the root mean squared error between the theoretical $R_t$ and median of the estimate shifted backwards in time. In Figure A3 panel B we quantify the delay between the theoretical and the estimated $R_t$, for a range of estimation methods. The delay is the result of the combination of estimate being a backward looking instantaneous reproduction number, which integrates information from the past according to the generation time distribution, and the method's estimation window, which in this case is 4, 7, or 14 days. From visual inspection of the time series in panel A there is no compelling evidence that the lag is significantly different from one time period to another, although there is a hint that the delay is shorter when case numbers are high and rising. For subsequent analysis each model's estimates of $R_t$ are adjusted backwards by the nearest integer number of days derived from this lag analysis. It must be pointed out that the true value of the reproduction number is calculated using the methods of Wallinga et al (2007) which produces a different type of reproduction number to the instantaneous reproduction number produced by the renewal equation methods presented here.

```{r}

p = lagAnalysisResult %>% lagPlot() 
p %>% saveThirdPageFigure(output("lagAnalysis"))

```

*Figure A3: Analysis of $R_t$ estimate time delays. In panel A as estimate of $R_t$ based on a periodic growth rate clearly shows estimate lag in all methods of estimating $R_t$. In panel B the root mean squared error for a range of values of lag are calculated. The minimum RMSE is depends on the estimation model employed, however there is not an obvious linear relationship.*

# Quantification of accuracy, bias and calibration

To investigate the bias and calibration of the estimation method we compare four statistics derived from the $R_t$ estimates and associated theoretical $R_t$ values, using the lag adjusted estimates. Estimates of $R_t$ include a mean, standard deviation and 2.5%, 5%, 25%, 50%, 75%, 95% and 97.5% quantiles. To identify bias, for each point we calculate the difference between median estimate and theoretical $R_t$ values as the residual ($\xi_{bias}$). On this measure a value of 0 represents an unbiased estimate, and bias is presented on the same scale as the $R_t$ estimate. 

To assess precision we measure the calibration of the estimate ($\xi_{cal}$). This is defined as 1 if the actual value is within the confidence intervals of the estimate, and 0 if the actual value is outside. A well calibrated set of estimates should have an average close to 1, and poorly calibrated estimates will be closer to zero.

We examine the confidence of the estimate using a continuous data analogy to the verification rank histogram [@andersonMethodProducingEvaluating1996; @brockerReliabilityAnalysisMulticategorical2008; @siegertSpecsVerificationForecastVerification2020]. The verification rank histogram examines the distribution of the ranks of each of the true values among the collection of associated point estimates. The shape of the distribution of these ranks determines the appropriateness of the confidence limits [@hamillInterpretationRankHistograms2001]. Appropriate variation in point estimates lead to a flat histogram with the true values falling evenly over the point estimates. Not enough variability in the point estimates results in a U-shaped histogram, too much in a dome shaped histogram. For our purposes we have individual estimates that describe the estimate as a a posterior distribution through a set of quantiles. This estimate is associated with a single true $R_t$ value. We can derive a cumulative density function of each estimate ($F(x)$) from the quantiles, by filling a monotonic Hermite spline to them, and use this CDF to estimate the quantile of the theoretical $R_t$ value with respect to the estimate distribution [$F(x = R_t)$]. The quantile of the actual $R_t$ value ($\xi_{quant}$) with regards to the posterior distribution of the $R_t$ estimate is equivalent to the rank in the rank histogram, and we define this as the "actual value quantile" for all the paired estimate distributions and true $R_t$ values. The density plot of the actual value quantile, for a range of estimates, we define as the quantile density plot and thsi has the same interpretation as the rank histogram.

The continuous ranked probability score (CRPS) is a measure of performance for probabilistic estimates of a scalar observation. It is a quadratic measure of the difference between the estimate cumulative distribution function (CDF)
and the empirical CDF of the observation [@zamoEstimationContinuousRanked2018]. Loosely speaking, it describes the mass of the probability density function that is closer to the mean of the estimate, than the true value is. For each estimate it can be calculated directly from the paired CDF of an estimate ($F(x)$) and a scalar true value ($y$) as the following, where $\mathbb{I}$ is the indicator function. This estimate-by-estimate value is referred to as the instantaneous CRPS ($\xi_{cprs}$), and low values imply higher quality estimates:

$$
\begin{aligned}
CRPS_{inst}(F,y) &= \int_{-\infty}^\infty \Big(F(x) - \mathbb{I}(x \ge y)\Big)^2dx\\
\end{aligned}
$$

In summary empirical distributions for the 4 metrics are derived from each individual estimate: 

$$
\begin{aligned}
\text{bias}: \xi_{bias} &= \Big[F_n^{-1}(0.5) - R_{t,n}\Big]\\
\text{calibration}: \xi_{cal} &= \Big[\mathbb{I}\Big(F_n^{-1}(0.025) \le R_{t,n} \le F_n^{-1}(0.975)\Big)\Big]\\
\text{actual value quantile}: \xi_{quant} &= \Big[F_n(x=R_{t,n})\Big]\\
\text{instantaneous continuous rank probability score}: \xi_{cprs} &= \Big[CPRS_{inst}(F_n,R_{t,n})\Big]\\
\end{aligned}
$$
In our case, of particular interest in assessing estimates of $R_t$ is the critical threshold of $R_t=1$ when an epidemic transitions from growth to decline or vice-versa. A specific measure for this scenario detects if the confidence limits of an estimate are both the opposite side of 1 to the true value. An ideal situation is that this never happens. This "critical threshold" metric ($\xi_{crit}$) is defined as follows assuming the sign function is defined as $sgn(x) = x/|x|$:

$$
\begin{aligned}
\text{critical threshold}: \xi_{crit} &= \Big[\mathbb{I}\Big(sgn(F_n^{-1}(0.025)-1) = sgn(R_{t,n}-1) || sgn(R_{t,n}-1) = sgn(F_n^{-1}(0.975)-1)\Big)\Big]\\
\end{aligned}
$$

When we wish to compare the performance of a given method against another we need aggregate the estimate level scores achieved against a particular validation data set. The median and inter-quartile ranges of the bias and instantaneous continuous rank probability score of all estimates can be simply calculated to give an overall metric for the performance of the estimation method. Likewise the calibration and critical threshold of each estimate can be aggregated into a single percentage for the method.

The actual value quantiles ($\xi_{quant}$), are inspected visually and have the same interpretation as the verification rank histogram, with a U-shaped density curve representing over-precision and a dome shaped density curve representing excessive uncertainty, and an ideal result being a uniform distribution. The average value of $\xi_{quant}$ should be close to 0.5. To quantify the shape of the quantile density plot, define the deviation of the values $\xi_{quant}$ for each of $N$ estimates from the expected value of 0.5, and compare this to the ideal value, which is the standard deviation of a uniform distribution with support between 0 and 1 ($\sqrt{\frac{1}{12}}$ = `r sprintf("%1.3f",sqrt(1/12))`). This gives us the following "quantile deviation" metric, which ranges between `r sprintf("%1.3f and %1.3f", 0.5-sqrt(1/12), -sqrt(1/12))` with positive values representing more deviation than expected, a U-shaped quantile density plot and over precise estimation methods, and negative values representing less deviation than expected, a dome shaped quantile density plot and excessive uncertainty in the estimation method:

$$
\text{quantile deviation}: Q_{dev} = \sqrt{\frac{\sum(\xi_{quant}-0.5)^2}{N-1}} - \sqrt{\frac{1}{12}}
$$

These metrics can be used to summarise the performance of each individual estimation method and compare it to others. At a top level we can aggregate the individual estimates over time and over the different simulations, to produce a single set of 5 metrics for each estimation method, with a focus on the overall performance. The quality metrics are visualised as box plots for the bias and CRPS, a simple proportion for the calibration, and critical threshold measure, and the distribution shape for the quantile density in the form of a violin plot. In this case the U shaped verification rank histogram becomes an I shaped quantile density violin plot, representing over-precision, and a dome shaped verification rank histogram, becomes an O shaped quantile density violin plot, representing excessive uncertainty. 

```{r}

p = estimateSummaryPlot(qualityAnalysisResult)
p %>% saveHalfPageFigure(output("errorSummaryComparison"))
```

*Figure A4: Estimate quality metric summaries for multiple estimation methods. In this instance we compare the performance of the renewal equation method with 14, 7, and 4 days as the windowing. Panel A show summary statistics for the bias of $R_t$ estimates, panel B shows the calibration. Panel C shows the quantile density, panel D the CRPS and panel E the critical threshold calibration.*

The top level comparison in Figure A4 shows similar performance of the different methods on most metrics. Unsurprisingly there is more spread in the error of estimates using a 4 day window seen in Panel A. All methods are similarly calibrated (panel B - higher is better) and show similar levels of over precision (Panel C). The CPRS (lower is better) is clearly higher for the 7 day and 4 day estimates, with the 14 day estimate performing best overall, reflecting the decrease in noise seen in panel A. 14 day model is also best at predicting growth when the epidemic is in decline and vice-versa, with less that 1% critical threshold error, although this is relatively good across all methods. The visual comparison is backed up by the quantitative metrics presented in table A1, which also gives us a quantitative comparison for the quantile divergence which confirms the over precision is worst (highest quantile deviation) for the 14 day estimates.

*Table A1: Estimate quality metric summaries for the three estimation methods compared here.*
```{r}
summaryAnalysis(qualityAnalysisResult,lagAnalysisResult) %>% standardPrintOutput::saveTable(output("errorSummaryComparisonTable"),defaultFontSize = 7)
```

To further assess the details of why the individual methods differ we investigate an intermediate level of detail, in which the metrics are broken down by differences in the simulated data as described below.

```{r}
p = estimateBreakdownPlot(qualityAnalysisResult, errorLimits = c(-0.15,0.15))
p %>% saveTwoThirdPageFigure(output("errorSummaryAnalysis"))
```

*Figure A5: Estimate quality metric intermediate detail and model comparison. Panel A-D show summary statistics for the bias of $R_t$ estimates broken down by the simulation smoothness, weekly variability, initial incidence, and time series boundary status. Panels E-F shows the calibration for the same subdivisions. Panels I-L shows the quantile density, panel M-P the CRPS, and panel Q-T the critical thresholds for the same subdivisions.*

At this intermediate detail the aggregation of the quality metrics are faceted by the salient features of the simulation, which inform us of where the estimators strengths and weaknesses are. These including the the smoothness of the growth rate function (first column), the degree of weekend effect (second column), initial size (third column), and also the proximity of the estimate to either end of the time series (final column). This last measure is defined as whether the estimate is within 21 days of the start or end of the time series. The latter is important because boundary effects particularly on most recent end of the time series may be important for real time estimates of $R_t$. This intermediate view of the quality of the estimates expands on the findings from figure A4, for example, in Panel G and K in Figure A5, we note the low calibration and excessive precision of all the estimates is due to the high incidence simulations. In panel O however we note that the CPRS is lower (better) in the higher incidence scenarios suggesting that the over-precise estimates do come with improved accuracy (also seen in panel C).

The boundary effect analysis in panels D,H,L,P demonstrates that all the methods investigated here are biased high in the first 3 weeks, and poorly calibrated. The critical threshold metric (panels Q,R,S & T) shows that the 14 day estimate method is able to detecting transitions between growth to decline well. The simulations with high weekly variation is seen to the accuracy of the 4 day estimate on this measure (panel R). The impact of the weekly variation on the 4 day estimate is also seen in panel B where increase spread of error occurs with increasing variation.

On the face of this comparison there is evidence to favour the 14 day estimate methodology.

To further understand the performance of the 14 day model, we can also visualize these metrics over time and between selected simulation configurations. For the single estimation methodology (14 days window) we can compare low and high incidence and smoothness of the simulation. We present in detail the 4 simulations with no weekend effects, 2 of which are based on smoothly varying growth rates, and 2 on stepped growth rates, with different numbers of initial cases (low incidence: 100 and high incidence: 10000).

```{r}
#TODO: this possibly throws error:
p = estimateTimeseriesPlot(qualityAnalysisResult, simFilterExpr = weekendEffect==0, modelFilterExpr = model=="EpiEstim: 14 day", labelExpr = paste0(smoothLabel,"; ",seedLabel))
suppressWarnings(
  p %>% saveFullPageFigure(output("errorAnalysis"))
)


```

*Figure A6: Time variations in estimate quality metrics for simulations with no weekly variability. Panel A shows the modelled case count, and one sample from the simulation for 4 synthetic data simulation configurations. Panel B shows the simulation reproduction number (red) and one estimate (black) based on one sample from the simulation, using the renewal equation method with a gamma distributed infectivity profile (mean 5 days, sd 4), and a fixed window of 14 days with the time corrected to account for estimate lags. Panel C shows the bias of each individual $R_t$ estimate and rolling quantiles over a 28 days period. Panel D shows the calibration of individual estimates and the rolling mean of the calibration (14 day window). Panel E shows the quantile density over time (with density as the shade), the rolling 10%, 30% 50% 70% and 90% quantiles are shown as grey lines calculated over a 28 day window. In panel F the instantaneous CRPS for individual estimates and the rolling quantiles are shown. In Panel G the critical threshold measure shows where the estimates confidence limits are the other side of the $R_t$ critical threshold of 1 to the actual value. In all panels quantiles shown in red are 2.5% and 97.5% if dotted, 25% and 75% if dashed and the median is a solid red line. Blue horizontal lines represent the median for the statistic for the whole time series.*

```{r}
tmp = summaryAnalysis(qualityAnalysisResult,lagAnalysisResult)
tmp = tmp %>% select(metric,value = `EpiEstim: 14 day`)
residualIQR = tmp %>% filter(metric=="average bias") %>% pull(value)
calibrationMean = tmp %>% filter(metric=="average calibration") %>% pull(value)
critThreshMean = tmp %>% filter(metric=="average critical threshold") %>% pull(value)
```

For this selection of simulation configurations and estimation method we again find that there is no evidence of strong systematic bias. Over all simulations in Figure A6 the bias is `r residualIQR` (reproduction number is unit-less). These is a suggestion in Panel C that this varies over time. The calibration of the estimates is very variable, and overall `r calibrationMean` of the estimates include the actual value in their confidence intervals. Panel D demonstrates that this is far worse when case numbers are high and during step changes in the reproduction number, which is due to over precision in the estimates. The quantile density (panel E) backs this up with the distribution swinging from one extreme to the other in all the parts of the simulations with high incidence, implying an over-precise estimate. However although the calibration is poor and the quantile density is far from a flat distribution, in the higher incidence simulations the bias of the estimates is smaller. When these observations are taken together, despite the over precision, we see in panel E that the CRPS is in fact lowest for the higher incidence simulations. Overall we can conclude that that this estimation methodology is most accurate with high case numbers but produces slightly biased estimates with excessive confidence. It performs less well when there is an abrupt change in $R_t$ (panel G), and this means that `r critThreshMean` of the estimates using this method incorrectly predict growth when in fact the epidemic is in decline, or vice-versa. 

# Summary

This appendix describes a methodology for verifying the estimation of $R_t$ using synthetic data sets designed to highlight particular issues, by comparing the posterior distributions of estimates to the known $R_t$ values used to generate the data sets. The different methods compared here have different degrees of delay between the actual $R_t$ using in creating the data set and the estimate ranging from 7.75 to 10.85 days. The performance of the estimation methods can be summarized with 5 metrics, the bias, calibration, quantile deviation, continuous ranked probability score, and the critical threshold measure. At a summary level the combination of these metrics allows performance of different estimate methods to be quantitatively compared, and demonstrate that despite a degree of over precision, and resulting lower calibration, the 14 day estimator performs best overall. Furthermore by comparing performance of different estimation methods against different simulations with specific features, we can qualitatively assess situations in which individual estimation methodologies perform better or worse. A detailed analysis of the time series of the metrics gives us further insight into situations where each given estimate method may under-perform. 

This validation methodology is extensible to other metrics other than $R_t$ where a set of confidence intervals or quantiles are available, particularly estimates of growth rate and incidence.

# References

<div id="refs"></div>