---
title: "Appendix A1 - Reproduction number estimate validation"
author: "Rob Challen"
date: '`r format(Sys.Date(), "%d-%m-%Y")`'
output: 
  pdf_document :
    fig_caption: yes
    keep_tex: TRUE
header-includes:
 \usepackage{float}
 \usepackage{mathtools}
 \usepackage{amsmath}
 \floatplacement{figure}{H}    
 \DeclareRobustCommand{\[}{\begin{equation*}}
 \DeclareRobustCommand{\]}{\end{equation*}}
 \newcounter{tagno}
 \setcounter{tagno}{0}
 \let\amsmathtag\tag
 \renewcommand{\tag}[1]{\amsmathtag{\thetagno} \label{#1} \stepcounter{tagno}}
knit: (function(inputFile, encoding,...) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "~/Dropbox/sarscov2/r-estimation-methodology", output_file=paste0('cori-method-validation-',Sys.Date(),'.pdf')) })
fig_width: 7
fig_height: 5
out.width: "100%"
bibliography: jepidemic.bib
csl: jepidemic.csl
vignette: >
  %\VignetteIndexEntry{Cori method}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  error = TRUE
)

here::i_am("vignettes/cori-method-validation.Rmd")
source(here::here("vignettes/common-setup.R"))
library(EpiEstim)
```

# Introduction

The reproduction number represents the ratio between the number of secondary cases resulting from each primary case. At the beginning of an outbreak assuming no prior immunity and a freely mixing population, this is described as the basic reproduction number, $R_0$. [CITE Vegvari] 

The effective reproduction number $R_t$, is a time varying quantity, which may be defined in terms of the basic reproduction number $R_0$, the fraction of contacts that people are making at a given time $C_t$, compared to a freely mixing population, and the fraction of the population that is still susceptible to infection $S_t$.

$$
R_t = S_t C_t R_0
$$
The infectivity profile is another probability distribution. Most often represented in discrete form $\omega_1, \omega_2, \dots, \omega_s$, that defines the likelihood that a case infected at time $t$ resulted from a case infected between the times $t-s$ and $t-s+1$. This definition implies that $\omega_{s \leq 0} = 0$ as that would apply to secondary infections resulting from primary infections in the future, and that the discrete time measure $s$ here represents the upper bound of the equivalent continuous unit time interval, rather than, for example, the middle of the interval.

Connecting to the instantaneous reproduction number and the infectivity profile is the quantity $\beta$ which is the "transmissibility" of an infection in an average individual, infected at a given time, $t$, at a given number of days post infection, $\tau$. $\beta$ is related to the in host viral load of an infection, and the number of contacts that infected individuals make with susceptible individuals.

$$
\beta_{t,s} = R_t\omega_s
$$

There are two basic types of effective reproduction number to consider. The simplest conceptually is the forward-looking definition, in which the reproduction number is the number of secondary infections generated by a single primary infection which occurs at time $t$, this is known as the case reproduction number $R_t^c$ (Frazer 2007) [CITE]. The case reproduction number reflects the state of the epidemic at a specific point in time but is limited by the fact that it can only be determined after the event. If we assume $I_0, I_1, \dots, I_t$ is a time series of infection counts, assumed to be drawn from some discrete probability distribution with expected value $\overline{I_t}$ then $R_t^c$ is given by: 

$$
\begin{aligned}
R_t^c &= \frac{
  \sum_{s=1}^\infty{\overline{I_{t+s}}}\omega_s
}{
  \overline{I_t}
}\\
\end{aligned}
$$

We can alternatively define the backward-looking effective reproduction number $R_t^i$ as the inverse ratio of the number of primary infections that cause the secondary infections observed at time $t$, this is known as the instantaneous reproduction number, $R_t^i$. In an evolving epidemic the instantaneous reproduction number is able to be calculated using data that has already been observed and is hence the more useful quantity. The rest of this summary considers the instantaneous version of the effective reproduction number, which we refer to as the reproduction number or $R_t$.

The Cori method, is a method used to calculate the reprodcution number which will be described in detail in later  has a reference implementation in the R package EpiEstim [CITE]. This allows for a variety of configuration options to suit different use cases. We concentate on the estimation of a time series of the reproduction number, for which the main parameters are a single fixed mean and standard deviation for the $R_t$ prior, the window over which the estimate will be performed, and an infectivity profile. The infectivity profile can be specified in a number of ways, but for our purposes we concentrate on the discrete empirical version ("non_parametric_si" option), using a single infectivity profile specified as part of the Flu2009 dataset. Figure A2.1 shows the behaviour of EpiEstim on an outbreak incidence bundled also included in the Flu2009 dataset for a range of different estimation windows.

```{r}
epiest = bind_rows(lapply(1:15, function(window) {
  tau = window-1
  tmp = EpiEstim::estimate_R(Flu2009$incidence, 
                             config = make_config(method = "non_parametric_si",
                                                  si_distr = Flu2009$si_distr, 
                                                  mean_prior=5, std_prior=4,
                                                  t_start=2:(32-tau),t_end=(2+tau):32))
  return(tmp$R %>% mutate(Rt.StartDate = tmp$date[t_start],
                          Rt.EndDate = tmp$date[t_end],
                          Rt.Mean = `Mean(R)`, 
                          Rt.Quantile.0.5 = `Median(R)`, 
                          Rt.Quantile.0.025 = `Quantile.0.025(R)`, 
                          Rt.Quantile.0.975 = `Quantile.0.975(R)`,
                          Rt.Window = window
  ))
}))
p1 = incidencePlot(Flu2009$incidence %>% mutate(date = dates, value = I, subgroup="none"))+xlab(NULL)
p2 = ggplot(tibble(y=Flu2009$si_distr) %>% mutate(x1=row_number()-1,x0=x1-1), aes(x=(x1+x0)/2,y=y))+geom_bar(stat="identity", width=0.9, colour="black",fill=NA)+scale_x_continuous(breaks=-1:14)+xlab("days since primary infection")+ylab("P(secondary infection on given day|secondary infection)")
p3 = rtPlot(epiest %>% filter(Rt.Window<=10),ylim = c(0,5))+facet_wrap(vars(Rt.Window),ncol=5)+standardPrintOutput::narrowAndTall()+xlab(NULL)

design = "
111122
333322
333322
"
p4 = p1+p2+p3+plot_annotation(tag_levels = "A")+plot_layout(design = design)
p4 %>% saveHalfPageFigure(output("epiEstimFlu2009"))
```

*Figure A2.1: Panel A shows a case count for the Flu2009 outbreak data set, Panel B shows the discrete infectivity profile, and panel C shows the $R_t$ time series estimates for a range of different estimation windows. Vertical error bars show the confidence in $R_t$ whereas horizontal error bars show the date range for which the assumption of constant $R_t$ for each estimate is applied*

This simple example demonstrates some features of the algorithm that we decided to further investigate, in that there is a clear trade-off between bias and variance in the window selection which is determined in part by the case counts within the time window estimate. With smaller estimate windows and less data at either end of the time series the estimates revert to the prior $R_t$ distribution, whereas with longer windows, at least in this scale and duration of outbreak, the detail is lost. There is a need to be able to assess the performance of an individual estimation methods (including parameterisation) against a standard and to be able to compare against each other. The purpose of this paper is to describe the validation procedure and define the associated quanity metrics we employ to compare estimates. This will be described in terms of $R_t$ estimation using three parameterisations of EpiEstim (14 day window, 7 day window and 4 day window) but is extensible to comparisons between other methods and to other observations we may wish to estimate such as case incidence, and growth rate as well.

```{r}
# <!-- ```{r} -->
J = jepidemic::JavaApi$new()
estim = J$CoriEstimator$new(r0Mean = 5,r0SD = 4,maxWindow = 14)
estim$withInfectivityProfile(infectivityProfile = Flu2009$si_distr, replace=FALSE)
# estim$detailedOutput() -->
estim$withDefaultPrior()
estim$atStartOfTimeseries()

# sudo R CMD javareconf JAVA_HOME=/usr/lib/jvm/java-9-oracle
# install.packages("rJava")
Sys.setenv('JAVA_HOME' = '/usr/lib/jvm/java-9-oracle/jre')
Sys.setenv('LD_LIBRARY_PATH' = '/usr/lib/jvm/java-9-oracle/lib/server')
rJava::.jinit()
rJava::.jcall("java/lang/System", "S", "getProperty", "java.runtime.version")
# 
# <!-- jepidem = estim$estimateRtSingle(incidence = Flu2009$incidence,dateColName = "dates", incidenceColName = "I") -->
# 
# <!-- comp = epiest %>% filter(is.finite(Rt.Mean)) %>% select(Rt.StartDate,Rt.EndDate,Rt.Window,Rt.Mean) %>% #,Rt.Quantile.0.5,Rt.Quantile.0.025,Rt.Quantile.0.975) %>% -->
# <!--   inner_join( -->
# <!--     jepidem %>% filter(is.finite(Rt.Mean)) %>% select(Rt.StartDate,Rt.EndDate,Rt.Window,Rt.Mean), #,Rt.Quantile.0.5,Rt.Quantile.0.025,Rt.Quantile.0.975), -->
# <!--     by=c("Rt.StartDate","Rt.EndDate","Rt.Window"), suffix=c(".epiest",".jepidem") -->
# <!-- ) -->
# 
# <!-- mismatches = comp %>% mutate(matches = abs(Rt.Mean.epiest - Rt.Mean.jepidem)<0.000000000001) %>% filter(isFALSE(matches)); -->
# 
# <!-- if (mismatches %>% nrow() != 0) { -->
# <!--   mismatches -->
# <!--   stop("mismatch between jepidemic and epiestim") -->
# <!-- } -->
# <!-- ``` -->
```


# Validation methodology

We construct synthetic data sets with known values of expected incidence, $R_t$ and exponential growth rate. This synthetic data set is generated using an initial case count, and a time series of exponential growth rate values. The growth rate time series are either a step function with 6 different predefined levels or a smooth cubic spline passing through 6 predefined control points, over the course of a theoretical year. The growth rate time series is accumulated and applied to the initial incidence to generate a time series of expected incidence. Theoretical values of $R_t$ are calculated from the growth rate using the methods of Wallinga et al (2007) [CITE] and assuming a synthetic generation time as a Gamma distribution with mean ($\mu$) of 5 days and standard deviation ($\sigma$) of 4, and use the following relationship, where $M$ is the moment generating function of the gamma distribution with shape parameter $\alpha = \mu^2/\sigma^2$ and rate parameter $\beta = \mu/\sigma^2$.

$$
\begin{aligned}
R_t &= \frac{1}{M(-r_t)} \\
&= \Big(1-\frac{r_t}{\beta}\Big)^{-\alpha} \\
&= \Big(1-\frac{r_t\sigma^2}{\mu}\Big)^{-\mu^2/\sigma^2}
\end{aligned}
$$

From each simulation of case incidence random bootstrap samples are drawn from a Poisson distribution whose rate is the expected incidence. Optionally a weekend effect is simulated, a second Poisson sample is made on Saturday, Sunday and Mondays in the time series with a rate given as a fraction of the expected incidence (0%, 3% or 10%). This second random sample is subtracted from the first on Saturdays and Sundays, and added on Mondays, giving a weekly cycle to case counts similar to that seen in reality. Any resulting negative values are set to zero. 

We simulate using one smooth and one stepped growth rate time series in combination with three levels of weekend effect mentioned above, and with two initial case counts (100 and 10,000), giving an overall 12 different configurations. The synthetic gamma distributed generation time is discretised on whole day intervals as expected by the algorithm. We assume the generation time is known precisely and does not vary for the synthetic data in this validation analysis.

```{r}
# <!-- To determine the expected uncertainty in our synthetic estimates we adopt a simulation approach. Assuming our simulation of the Poisson rate is the ground truth, observation of incidence are random samples from the associated Poisson distribution. We assert that estimates made from these samples, given that in real life there is only one observation per day, should have a uncertainty at least in the same order of magnitude as the variance of the sampling distribution, which is determined by the Poisson rate itself. If we extend this logic to the growth rate, which is the gradient of the logarithm of the Poisson rate. -->
# 
# <!-- $$ -->
# <!-- \begin{aligned} -->
# <!-- P(I_t=k) &= \frac{\lambda_t^ke^{-\lambda_t}}{k!} \\ -->
# <!-- Q(\lambda) &= \frac{\lambda+\Delta\lambda}{\lambda} \\ -->
# <!-- r_t &= \frac{d}{dt}log(\lambda_t) \\ -->
# <!-- &\approx \frac{1}{2}log\frac{\lambda_{t+1}}{\lambda_{t-1}}\\ -->
# <!-- \end{aligned} -->
# <!-- $$ -->
# <!-- $$ -->
# <!-- P(R<r_t) = P(\lambda_{t+1} < 2e^{r_t}\lambda_{t-1}) -->
# <!-- $$ -->
# 
# 
# 
# <!-- $$ -->
# <!-- \begin{aligned} -->
# <!-- \Delta r_t&= \frac{1}{2}\log\frac{Q(\lambda_{t+1})}{Q(\lambda_{t-1})}\\ -->
# <!-- \lambda_{t+k} &= \lambda_te^{kr_t} \\ -->
# <!-- Q(\lambda_{t+k}) &= \frac{\lambda_te^{kr_t} + \Delta \lambda_{t+1}}{\lambda_te^{kr_t}}\\ -->
# <!-- R_t &= \Big(1-\frac{r_t}{\beta}\Big)^{-\alpha} \\ -->
# <!-- r_t &= \beta(1-R_t^\alpha) \\ -->
# 
# <!-- \end{aligned} -->
# <!-- $$ -->
# <!-- TODO: sampling. -->
# <!-- Assume lambda_t is a poisson. -->
# <!-- Sample from lambda_t to derive CI's -->
# <!-- use finite differences 1/2 log lambda1 - log lambda2 = growth rate  -->
# <!-- to relate lambda1 to lambda 2 given lambda and growth rate. -->
# <!-- sample lambda1 & lambda2 for growth rate -->
# 
# <!-- Rt = lamda1 / sum(omega * lamda_t-s) -->
# <!-- Assume lamda2 = lamda t-s. lamda1 = lambda2*Rt -->
# <!-- sample for CIs -->
```

```{r}


rtEstimator = function(window, estimator=estim) {
  return(function(ts, infectivityProfile) {
    estimator$withInfectivityProfileMatrix(infectivityProfile, replace = TRUE)
    estimator$selectSpecificWindow(window)
    estimator$estimateRt(ts %>% group_by(subgroup),dateColName = "date",incidenceColName = "value")
  })
}

estimators = tibble(
  model = forcats::as_factor(c(
    "EpiEstim: 4 day",
    "EpiEstim: 7 day",
    "EpiEstim: 14 day"
  )), 
  estimFn = c(
    rtEstimator(4),
    rtEstimator(7),
    rtEstimator(14)
  )
)

# stepTs = synthetic %>% filter(weekendEffect == 0 & smooth==TRUE & seed == 100) %>% pull(ts) %>% head(1) %>% `[[`(1)
# stepRt = estim$estimateRt(stepTs %>% group_by(subgroup),dateColName = "date",incidenceColName = "value")
#syntheticEstimates = synthetic %>% mutate(estimate = map2(.x = ts, .y=infectivityProfile., .f=generateEstimate, estim=estim))

# syntheticEstimates = synthetic %>% group_by(weekendEffect,smooth,control,seed,infectivityProfile) %>% group_modify(function(d,g,...) {
#   estim$withInfectivityProfile(g$infectivityProfile[[1]], replace = TRUE)
#   estim$selectSpecificWindow(14)
#   out = estim$estimateRt(d$ts[[1]] %>% group_by(subgroup),dateColName = "date",incidenceColName = "value")
#   return(tibble(model = "EpiEstim: 14 day", estimate = list(out), ts = d$ts))
# }) 

lagAnalysisResult = lagAnalysis(estimators)
qualityAnalysisResult = qualityAnalysis(estimators, lagAnalysisResult$modelLag)

# syntheticEstimates = synthetic %>% inner_join(estimators, by=character()) %>% mutate(
#   estimate = pmap(.l=list(fn = estimFn, ts = ts, infectivityProfile = infectivityProfile), .f = function(fn,ts,infectivityProfile) {
#     fn(ts,infectivityProfile)
#   })
# )


```

```{r}

# tmp = syntheticEstimates %>% 
#   filter(weekendEffect == 0.1 & seed == 100) %>%
#   mutate(label = paste0(smoothLabel,"; ",seedLabel,"; ",weekendLabel))
#   
# tmp2 = tmp %>% ungroup() %>% select(label,ts) %>% distinct() %>% unnest(ts) %>% filter(subgroup==1) %>% mutate(row="simulation")
# tmp3 = tmp %>% ungroup() %>% select(model,label,estimate) %>% unnest(estimate) %>% filter(subgroup==1)
# 
# p1 = incidencePlot(tmp2)+geom_line(aes(y=lambda_t),colour="red")+facet_grid(rows = vars(row), cols=vars(label))
# 
# p2 = rtRibbon(tmp3,ylim = c(0.6,1.4), aes(group=subgroup))+
#   geom_line(data=tmp2,mapping=aes(x=date,y=Rt.actual),colour="red")+
#   ylab(latex2exp::TeX("$R_t$ estimate"))+xlab(NULL) +facet_grid(rows = vars(model),cols = vars(label))
# 
# p3 = p1+standardPrintOutput::hideX() + p2 +patchwork::plot_layout(ncol=1, heights=c(1,4))+patchwork::plot_annotation(tag_levels = "A")

p3 = qualityAnalysisResult %>% estimationExamplePlot(subgroups = 2)
p3 %>% saveHalfPageFigure(output("epiEstimSynthetic14day"))

```

*Figure A2.2: Panel A shows a case count, for a synthetic data sets generated from a spline function for growth rate, with low (100) initial case counts, and a 10% weekly variability. Expected incidence curves are shown in red and black points represent one set of data samples (out of 10 generated). Panel B shows the estimated reproduction number using the Cori method with a gamma distributed infectivity profile (mean 5 days, sd 4), and a fixed window of 14 days. The red line shows the theoretical $R_t$ value.*

In Figure A2.2 we show 1 sample from the smooth test configuration, with 10% weekly variation and 100 initial cases. The reproduction number is estimated with the Cori method in its default configuration with a fixed window of 14 days. Estimate uncertainty is larger when case numbers are low. The estimates (black lines in panel B) are clearly lagged compared to the theoretical $R_t$ value, particularly noticeably when case counts are high. At the beginning of the time series the estimate is consistently inappropriately high reflecting the prior distribution of the $R_t$ estimate, and a boundary effect at the beginning of the time series. Although only one time series from each configuration are being shown here, 10 are generated, to simulate data variability. Similarly we are showing one simulation configuration here as an example, but overall we have 12 configurations which describe different combinations of smoothness, weekend effect, and initial incidence.

# Quantifying estimate delay

Before we can assess the quality of the estimates we need to determine how much delay there is in the estimates. To do this we use another synthetic data set which is generated from a triangular wave for growth rate with period of 91 days. For a range of different time lags (0 days to 28 days) we calculate the root mean squared error between the theoretical $R_t$ and median of the estimate shifted backwards in time. In Figure A2.4 panel B we quantify the delay between the theoretical and the estimated $R_t$, for a range of estimation methods. The delay is the result of the combination of estimate being a backward looking instantaneous reproduction number, which integrates information from the past according to the generation time distribution, and the method's estimation window, which in this case is 4, 7, or 14 days. From visual inspection of the time series in panel A there is no compelling evidence that the lag is significantly different from one time period to another, although there is a hint that the delay is shorter when case numbers are high and rising. For subsequent analysis each model's estimates of $R_t$ are adjusted backwards by the nearest integer number of days derived from this lag analysis.

```{r}



lagAnalysisResult %>% lagPlot() %>% saveThirdPageFigure(output("lagAnalysis"))

## use a slider to compare quantile model to actual RT over a range of days before and after
# periodicCDF2 = periodicCDF %>% group_by(subgroup) %>% arrange(date) %>%
#   inner_join(periodicTs %>% select(subgroup,date,Rt.actual), by=c("subgroup","date")) %>%
#   # This gives the Rt.actual as a quantile of the predicted Rt distribution by sline interpolatng on the CDF
#   mutate(quantile.window = slider::slide2(.x = Rt.actual, .y = quantile.model, .f = ~ tibble(lag=14:-14, q = .y[[15]](.x)), .before = 14, .after = 14, .complete = TRUE))
# 
# # stepCDF2$quantile.window %>% last() %>% glimpse()
# # estimate the lag needed for each data point for the estimate to line up with an actual value of R_t
# lags = periodicCDF2 %>% select(date, subgroup, quantile.window) %>% 
#   mutate(lag = 
#     map_dbl(quantile.window, function(data) {
#       if (all(is.na(data$q))) return(NA_real_)
#       if (all(data$q-0.5 > 0)) return(NA_real_)
#       if (all(data$q-0.5 < 0)) return(NA_real_)
#       fn = splinefun(x=data$lag,y=data$q-0.5,method = "monoH.FC")
#       out = tryCatch({
#         uniroot(fn, lower = -14, upper =14)$root
#       }, error=function(e) NA_real_)
#       return(out)
#     })
#   )
# 
# p1 = ggplot(lags, aes(x=date,y=lag))+geom_point(size=0.1)+
#   #plotRollingQuantiles(lags %>% ungroup(), lag)+
#   geom_hline(yintercept=0,colour="grey50")
# 
# p1+standardPrintOutput::hideX()+p2+xlab(NULL)+plot_layout(ncol=1,heights = c(2,1))
# p1 %>% saveThirdPageFigure(output("lagAnalysis"))

# printIQR(lags$lag)
```

*Figure A2.3: Analysis of $R_t$ estimate time delays. In panel A as estimate of $R_t$ based on a periodic growth rate clearly shows estimate lag in all methods of estimating $R_t$. In panel B the root mean squared error for a range of values of lag are calculated. The minimum RMSE is depends on the estimation model employed, however there is not an obvious linear relationship.*

# Quantification of accuracy, bias and calibration

To investigate the bias and calibration of the estimation method we compare four statistics derived from the $R_t$ estimates and associated theoretical $R_t$ values, using the lag adjusted estimates. Estimates of $R_t$ include a mean, standard deviation and 2.5%, 5%, 25%, 50%, 75%, 95% and 97.5% quantiles. To identify bias, for each point we calculate the difference between median estimate and theoretical $R_t$ values as the residual. On this measure a value of 0 represents an unbiased estimate, and bias is presented on the same scale as the $R_t$ estimate. To assess precision we measure the calibration of the estimate. This is defined as 1 if the actual value is within the confidence intervals of the estimate, and 0 if the actual value is outside. A well calibrated set of estimates should have an average close to 1, and poorly calibrated estimates will be closer to zero.

We examine the confidence of the estimate using a continuous data analogy to the verification rank histogram [CITE Anderson 1996, Brocker 2008, Seighert 2020 <specs verification>]. The verification rank histogram examines the distribution of the ranks of each of the true values among the collection of associated point estimates. The shape of the distribution of these ranks determines the appropriateness of the confidence limits. Appropriate variation in point estimates lead to a flat histogram with the true values falling evenly over the point estimates. Not enough variability in the point estimates results in a U-shaped histogram, too much in a dome shaped histogram. For our purposes we have individual estimates that describe a distributional form through a set of quantiles. This distribution is associated with a single true $R_t$ value. We can derive a cumulative density function ($F(x)$) from our estimate quantiles, by filling a monotonic Hermite spline to them, and use this CDF to estimate the quantile of the theoretical $R_t$ value with respect to the estimate distribution [$F(x = R_t)$]. The quantile of the true $R_t$ value is equivalent to the rank in the rank histogram, and for all the paired estimate distributions and true $R_t$ values we plot the density of the resulting quantiles. The shape of this plot has the same interpretation as the verification rank histogram.

The continuous ranked probability score (CRPS) is a measure of performance for probabilistic estimates of a scalar observation. It is a quadratic measure of the difference between the estimate cumulative distribution function (CDF)
and the empirical CDF of the observation [CITE Zamo 2018]. Loosely speaking, it describes the mass of the probability density function that is closer to the mean of the estimate, than the true value is. For each estimate it can be calculated directly from the paired CDF of an estimate ($F(x)$) and a scalar true value ($y$) as the following, where $\mathbb{I}$ is the indicator function. This estimate-by-estimate value is referred to as the instantaneous CRPS, and low values imply higher quality estimates:

$$
\begin{aligned}
CRPS_{inst}(F,y) &= \int_{-\infty}^\infty \Big(F(x) - \mathbb{I}(x \ge y)\Big)^2dx\\
\end{aligned}
$$

In summary empirical distributions for the 4 metrics are derived from all estimates and aggregated to summary statistics, or visualised in various ways as described below: 

$$
\begin{aligned}
\text{bias}: \xi_{bias} &= \Big[F_n^{-1}(0.5) - R_{t,n}\Big]\\
\text{calibration}: \xi_{cal} &= \Big[\mathbb{I}\Big(F_n^{-1}(0.025) \le R_{t,n} \le F_n^{-1}(0.975)\Big)\Big]\\
\text{quantile density}: \xi_{quant} &= \Big[F_n(x=R_{t,n})\Big]\\
\text{continuous rank probability score}: \xi_{cprs} &= \Big[CPRS_{inst}(F_n,R_{t,n})\Big]\\
\end{aligned}
$$
Of particular interest in assessing estimates of $R_t$ is the critical threshold of $R_t=1$ when an epidemic transitions from growth to decline or vice-versa. A specific measure for this scenario detects if the confidence limits of an estimate are both the opposite side of 1 to the true value. This "critical threshold" metric is defined as follows assuming the sign function is defined as $sgn(x) = x/|x|$:

$$
\begin{aligned}
\text{critical threshold}: \xi_{crit} &= \Big[\mathbb{I}\Big(sgn(F_n^{-1}(0.025)-1) = sgn(R_{t,n}-1) || sgn(R_{t,n}-1) = sgn(F_n^{-1}(0.975)-1)\Big)\Big]\\
\end{aligned}
$$


```{r}

# syntheticEstimates2 = syntheticEstimates %>% 
#   mutate(
#     estimate = map(estimate, inferQuantiles)
#   ) %>%
#   inner_join(lagAnalysisResult$modelLag, by="model") %>%
#   mutate(comparison = map2(estimate,ts, function(x, y) {
#     x %>% mutate(
#       Rt.estimate.date = date,
#       date = Rt.estimate.date-round(medianLag),
#       boundaryEffect = case_when(
#         date < min(date)+21 ~ "start",
#         date >= max(date)-21 ~ "end",
#         TRUE ~ "middle"
#       ) %>% ordered(levels=c("start","middle","end"))
#     ) %>%
#     inner_join(y %>% select(subgroup,date,Rt.actual, lambda_t), by=c("subgroup","date")) %>%
#     mutate(
#       quantile.actual = map2_dbl(quantile.model, Rt.actual, ~.x(.y)),
# #      Rt.samples = map(inv.quantile.model, ~.x(runif(n=100))),
#       Rt.residual = Rt.Median-Rt.actual,
#       Rt.residual.sqrd = (Rt.Mean-Rt.actual)^2,
# #      Rt.relative = Rt.Median/Rt.actual,
#       Rt.calibration = ifelse(Rt.actual > Rt.Lo & Rt.actual < Rt.Hi,1,0),
#       Rt.critical_threshold = ifelse(sign(Rt.actual-1)!=sign(Rt.Lo-1) & sign(Rt.actual-1)!=sign(Rt.Hi),1,0),
#       quantile.residual = 0.5-quantile.actual,
#       Rt.residual.abs = abs(Rt.actual-Rt.Median),
#       quantile.residual.abs = abs(0.5-quantile.actual)
#     ) %>% mutate(
# #      Rt.relative.percent = (Rt.relative-1)*100,
#       quantile.probability = 1-quantile.residual.abs*2,
# #      Rt.residual.quantile = rank(Rt.residual.abs)/n(),
#       Rt.crps = pmap_dbl(
#         .l = list(F = quantile.model, limits = quantile.model.range, actual = Rt.actual), 
#         .f = function(F,limits,actual) {
#             tmp = integrate(f = function(x) (F(x)-(x >= actual))^2, lower = limits[1], upper=limits[2])
#             return(tmp$value)
#           }
#         )
#     )
#   }))

# stepCDF = inferQuantiles(stepRt)
# stepCDF3 = stepCDF %>% mutate(
#     Rt.estimate.date = date,
#     date = Rt.estimate.date-medianLag
#   ) %>%
#   inner_join(stepTs %>% select(subgroup,date,Rt.actual, lambda_t), by=c("subgroup","date")) %>%
#   mutate(
#     quantile.actual = map2_dbl(quantile.model, Rt.actual, ~.x(.y)),
#     # Rt.Median = map_dbl(quantile.data, ~ .x  %>% filter(q==0.5) %>% pull(x)), 
#     Rt.residual = Rt.Median-Rt.actual,
#     Rt.relative = Rt.Median/Rt.actual,
#     quantile.residual = 0.5-quantile.actual,
#     Rt.residual.abs = abs(Rt.actual-Rt.Median),
#     quantile.residual.abs = abs(0.5-quantile.actual)
#   ) %>% mutate(
#     Rt.relative.percent = (Rt.relative-1)*100,
#     quantile.probability = 1-quantile.residual.abs*2
#   )
```

These metrics can be used to summarise the performance of an individual estimation methods and compare it to others. At a top level we can aggregate the individual estimates over time and over the different simulations, to produce a single set of 5 metrics for each estimation method, with a focus on the overall performance. The quality metrics are visualised, as box plots for the bias and CRPS, a simple proportion for the calibration, and critical threshold measure, and the distribution shape for the quantile density in the form of a violin plot. 

```{r}
# calibration = function(groupedDf,var = "Rt.calibration") {
#   var = ensym(var)
#   groupedDf %>% summarise(!!var := sum(!!var,na.rm=TRUE), N = n()) %>%
#     mutate(middle = !!var/N)
# }
# 
# boxplotData = function(groupedDf, var) {
#   var = ensym(var)
#   groupedDf %>% summarise(
#     tibble(
#       names = c("ymin","lower","middle","upper","ymax"),
#       values = boxplot.stats(!!var, do.conf = FALSE, do.out = FALSE)$stats
#     )
#   ) %>% pivot_wider(names_from = names, values_from = values)
# }
# 
# 
# tmp6 = syntheticEstimates2 %>% 
#   ungroup() %>%
#   select(model,weekendLabel,seedLabel,smoothLabel,comparison) %>% 
#   unnest(comparison) 
# 
# # residual error
# p1 = ggplot(tmp6 %>% group_by(model) %>% boxplotData(Rt.residual), aes(ymin=ymin,lower=lower,middle=middle,upper=upper,ymax=ymax,x=model, colour=model))+geom_boxplot(stat="identity",size=0.2)+coord_cartesian(ylim = c(-0.1,0.1))+xlab(NULL)+ylab("bias")
# # calibration
# p2 = ggplot(tmp6 %>% group_by(model) %>% calibration(),aes(x=model, y=middle,colour=model))+geom_bar(stat="identity",fill="white",size=0.2,position="dodge")+coord_cartesian(ylim = c(0,1))+ylab("calibration")+xlab(NULL)
# # Rank histogram / Quantile density
# p3 = ggplot(tmp6, aes(y=quantile.actual, x=model, colour=model))+geom_violin(bw=0.025,size=0.2)+xlab(NULL)+ylab("quantile density")
# # Continuous ranked probability score
# p4 = ggplot(tmp6 %>% group_by(model) %>% boxplotData(Rt.crps), aes(ymin=ymin,lower=lower,middle=middle,upper=upper,ymax=ymax,x=model, colour=model))+geom_boxplot(stat="identity",size=0.2)+xlab(NULL)+ylab("CRPS")+coord_cartesian(ylim = c(0,0.05))
# p5 = ggplot(tmp6 %>% group_by(model) %>% calibration(Rt.critical_threshold),aes(x=model, y=middle,colour=model))+geom_bar(stat="identity",fill="white",size=0.2,position="dodge")+ylab("crit threshold")+xlab(NULL)
# 
# p = p1+guides(colour=guide_none())+standardPrintOutput::hideX()+
#   p2+standardPrintOutput::smallLegend(spaceLegend = 1)+guides(colour=guide_legend(title=NULL))+standardPrintOutput::hideX()+
#   p3+guides(colour=guide_none())+standardPrintOutput::hideX()+
#   p4+guides(colour=guide_none())+standardPrintOutput::hideX()+
#   p5+guides(colour=guide_none())+standardPrintOutput::hideX()+
#   patchwork::guide_area()+
#   patchwork::plot_annotation(tag_levels = "A")+
#   patchwork::plot_layout(ncol=3,guides = "collect")
p = estimateSummaryPlot(qualityAnalysisResult)
p %>% saveHalfPageFigure(output("errorSummaryComparison"))
```

*Figure A2.4: Estimate quality metric summaries for multiple estimation methods. In this instance we compare the performance of the Cori method with 14, 7, and 4 days as the windowing. Panel A show summary statistics for the bias of $R_t$ estimates, panel B shows the calibration. Panel C shows the quantile density and panel D the CRPS.*

The top level comparison in Figure A2.4 shows similar performance of the different methods on most metrics. Unsurprisingly there is more spread in the error of estimates using a 4 day window seen in Panel A. All methods are similarly calibrated (panel B) and show similar levels of over precision (Panel C). The CPRS is clearly higher for the 7 day and 4 day estimates, with the 14 day estimate performing best overall, reflecting the decrease in noise seen in panel A. The 14 day model however is also the worst at predicting growth when the epidemic is in decline and vice-versa, although this is relatively uncommon in all methods. To further assess the details of why the individual models differ we investigate an intermediate level of detail, in which the metrics are broken down by differences in the simulated data as described below.

```{r}
# tmp5 = syntheticEstimates2 %>% 
#   #filter(model == "EpiEstim: 14 day") %>%
#   ungroup() %>%
#   select(model,weekendLabel,seedLabel,smoothLabel,comparison) %>% 
#   unnest(comparison) 
# 
# # calibration = function(groupedDf) {
# #   groupedDf %>% summarise(Rt.calibrated = sum(Rt.calibration), N = n()) %>%
# #     mutate(binom::binom.confint(Rt.calibrated, N, methods="wilson")) %>%
# #     rename(ymin = lower,ymax = upper) %>% select(-mean) %>%
# #     mutate(binom::binom.confint(x,n,methods="wilson",conf.level=0.5)) %>%
# #     rename(middle=mean)
# # }
# 
# 
# 
# 
# #tmp7 = tmp5 %>% group_by(model,smooth) %>% calibration()
# # ggplot(tmp5 %>% group_by(model,smooth) %>% calibration(),aes(x=smooth, middle=middle,lower=lower,upper=upper,ymin=ymin,ymax=ymax,colour=model))+geom_boxplot(stat="identity",outlier.shape = NA)+coord_cartesian(ylim = c(0,1))
# # ggplot(tmp5 %>% group_by(model,weekendLevel) %>% calibration(),aes(x=weekendLevel, middle=middle,lower=lower,upper=upper,ymin=ymin,ymax=ymax,colour=model))+geom_boxplot(stat="identity",outlier.shape = NA)+coord_cartesian(ylim = c(0,1))
# # ggplot(tmp5 %>% group_by(model,seed) %>% calibration(),aes(x=seed, middle=middle,lower=lower,upper=upper,ymin=ymin,ymax=ymax,colour=model))+geom_boxplot(stat="identity",outlier.shape = NA)+coord_cartesian(ylim = c(0,1))
# 
# 
# 
# # residual error
# p1 = ggplot(tmp5 %>% group_by(model,smoothLabel) %>% boxplotData(Rt.residual), aes(ymin=ymin,lower=lower,middle=middle,upper=upper,ymax=ymax,x=smoothLabel, colour=model))+geom_boxplot(stat="identity",size=0.2)+coord_cartesian(ylim = c(-0.1,0.1))+xlab(NULL)+ylab("bias")
# p2 = ggplot(tmp5 %>% group_by(model,weekendLabel) %>% boxplotData(Rt.residual), aes(ymin=ymin,lower=lower,middle=middle,upper=upper,ymax=ymax,x=weekendLabel, colour=model))+geom_boxplot(stat="identity",size=0.2)+coord_cartesian(ylim = c(-0.1,0.1))+xlab(NULL)+ylab("bias")
# p3 = ggplot(tmp5 %>% group_by(model,seedLabel) %>% boxplotData(Rt.residual), aes(ymin=ymin,lower=lower,middle=middle,upper=upper,ymax=ymax,x=seedLabel, colour=model))+geom_boxplot(stat="identity",size=0.2)+coord_cartesian(ylim = c(-0.1,0.1))+xlab(NULL)+ylab("bias")
# p4 = ggplot(tmp5 %>% group_by(model,boundaryEffect) %>% boxplotData(Rt.residual), aes(ymin=ymin,lower=lower,middle=middle,upper=upper,ymax=ymax,x=boundaryEffect, colour=model))+geom_boxplot(stat="identity",size=0.2)+coord_cartesian(ylim = c(-0.1,0.1))+xlab(NULL)+ylab("bias")
# 
# # calibration
# p5 = ggplot(tmp5 %>% group_by(model,smoothLabel) %>% calibration(),aes(x=smoothLabel, y=middle,colour=model))+geom_bar(stat="identity",fill="white",size=0.2,position="dodge")+coord_cartesian(ylim = c(0,1))+ylab("calibration")+xlab(NULL)
# p6 = ggplot(tmp5 %>% group_by(model,weekendLabel) %>% calibration(),aes(x=weekendLabel, y=middle,colour=model))+geom_bar(stat="identity",fill="white",size=0.2,position="dodge")+coord_cartesian(ylim = c(0,1))+ylab("calibration")+xlab(NULL)
# p7 = ggplot(tmp5 %>% group_by(model,seedLabel) %>% calibration(),aes(x=seedLabel, y=middle,colour=model))+geom_bar(stat="identity",fill="white",size=0.2,position="dodge")+coord_cartesian(ylim = c(0,1))+ylab("calibration")+xlab(NULL)
# p8 = ggplot(tmp5 %>% group_by(model,boundaryEffect) %>% calibration(),aes(x=boundaryEffect, y=middle,colour=model))+geom_bar(stat="identity",fill="white",size=0.2,position="dodge")+coord_cartesian(ylim = c(0,1))+ylab("calibration")+xlab(NULL)
# 
# # Rank histogram / Quantile density
# p9 = ggplot(tmp5, aes(y=quantile.actual, x=smoothLabel, colour=model))+geom_violin(bw=0.025,size=0.2)+xlab(NULL)+ylab("quant. dens.")
# p10 = ggplot(tmp5, aes(y=quantile.actual, x=weekendLabel, colour=model))+geom_violin(bw=0.025,size=0.2)+xlab(NULL)+ylab("quant. dens.")
# p11 = ggplot(tmp5, aes(y=quantile.actual, x=seedLabel, colour=model))+geom_violin(bw=0.025,size=0.2)+xlab(NULL)+ylab("quant. dens.")
# p12 = ggplot(tmp5, aes(y=quantile.actual, x=boundaryEffect, colour=model))+geom_violin(bw=0.025,size=0.2)+xlab(NULL)+ylab("quant. dens.")
# 
# # Continuous ranked probability score
# p13 = ggplot(tmp5 %>% group_by(model,smoothLabel) %>% boxplotData(Rt.crps), aes(ymin=ymin,lower=lower,middle=middle,upper=upper,ymax=ymax,x=smoothLabel, colour=model))+geom_boxplot(stat="identity",size=0.2)+xlab(NULL)+ylab("CRPS")+coord_cartesian(ylim = c(0,0.05))
# p14 = ggplot(tmp5 %>% group_by(model,weekendLabel) %>% boxplotData(Rt.crps), aes(ymin=ymin,lower=lower,middle=middle,upper=upper,ymax=ymax,x=weekendLabel, colour=model))+geom_boxplot(stat="identity",size=0.2)+xlab(NULL)+ylab("CRPS")+coord_cartesian(ylim = c(0,0.05))
# p15 = ggplot(tmp5 %>% group_by(model,seedLabel) %>% boxplotData(Rt.crps), aes(ymin=ymin,lower=lower,middle=middle,upper=upper,ymax=ymax,x=seedLabel, colour=model))+geom_boxplot(stat="identity",size=0.2)+xlab(NULL)+ylab("CRPS")+coord_cartesian(ylim = c(0,0.05))
# p16 = ggplot(tmp5 %>% group_by(model,boundaryEffect) %>% boxplotData(Rt.crps), aes(ymin=ymin,lower=lower,middle=middle,upper=upper,ymax=ymax,x=boundaryEffect, colour=model))+geom_boxplot(stat="identity",size=0.2)+xlab(NULL)+ylab("CRPS")+coord_cartesian(ylim = c(0,0.05))
# 
# p17 = ggplot(tmp5 %>% group_by(model,smoothLabel) %>% calibration(Rt.critical_threshold),aes(x=smoothLabel, y=middle,colour=model))+geom_bar(stat="identity",fill="white",size=0.2,position="dodge")+ylab("crit. thresh.")+xlab(NULL)+coord_cartesian(ylim = c(0,0.015))
# p18 = ggplot(tmp5 %>% group_by(model,weekendLabel) %>% calibration(Rt.critical_threshold),aes(x=weekendLabel, y=middle,colour=model))+geom_bar(stat="identity",fill="white",size=0.2,position="dodge")+ylab("crit. thresh.")+xlab(NULL)+coord_cartesian(ylim = c(0,0.015))
# p19 = ggplot(tmp5 %>% group_by(model,seedLabel) %>% calibration(Rt.critical_threshold),aes(x=seedLabel, y=middle,colour=model))+geom_bar(stat="identity",fill="white",size=0.2,position="dodge")+ylab("crit. thresh.")+xlab(NULL)+coord_cartesian(ylim = c(0,0.015))
# p20 = ggplot(tmp5 %>% group_by(model,boundaryEffect) %>% calibration(Rt.critical_threshold),aes(x=boundaryEffect, y=middle,colour=model))+geom_bar(stat="identity",fill="white",size=0.2,position="dodge")+ylab("crit. thresh.")+xlab(NULL)+coord_cartesian(ylim = c(0,0.015))
```

```{r}
# p = p1+standardPrintOutput::hideX()+guides(colour=guide_none())+
#   p2+standardPrintOutput::hideX()+standardPrintOutput::hideY()+guides(colour=guide_none())+
#   p3+standardPrintOutput::hideX()+standardPrintOutput::hideY()+guides(colour=guide_none())+
#   p4+standardPrintOutput::hideX()+standardPrintOutput::hideY()+guides(colour=guide_none())+
#   p5+standardPrintOutput::hideX()+standardPrintOutput::smallLegend(spaceLegend = 0.75)+guides(colour=guide_legend(title=NULL))+
#   p6+standardPrintOutput::hideX()+standardPrintOutput::hideY()+guides(colour=guide_none())+
#   p7+standardPrintOutput::hideX()+standardPrintOutput::hideY()+guides(colour=guide_none())+
#   p8+standardPrintOutput::hideX()+standardPrintOutput::hideY()+guides(colour=guide_none())+
#   p9+standardPrintOutput::hideX()+guides(colour=guide_none())+
#   p10+standardPrintOutput::hideX()+standardPrintOutput::hideY()+guides(colour=guide_none())+
#   p11+standardPrintOutput::hideX()+standardPrintOutput::hideY()+guides(colour=guide_none())+
#   p12+standardPrintOutput::hideX()+standardPrintOutput::hideY()+guides(colour=guide_none())+
#   p13+standardPrintOutput::hideX()+guides(colour=guide_none())+
#   p14+standardPrintOutput::hideX()+standardPrintOutput::hideY()+guides(colour=guide_none())+
#   p15+standardPrintOutput::hideX()+standardPrintOutput::hideY()+guides(colour=guide_none())+
#   p16+standardPrintOutput::hideX()+standardPrintOutput::hideY()+guides(colour=guide_none())+
#   p17+guides(colour=guide_none())+
#   p18+standardPrintOutput::hideY()+guides(colour=guide_none())+
#   p19+standardPrintOutput::hideY()+guides(colour=guide_none())+
#   p20+standardPrintOutput::hideY()+guides(colour=guide_none())+
#   patchwork::plot_annotation(tag_levels = "A")+
#   patchwork::plot_layout(ncol=4,guides = "collect")
p = estimateBreakdownPlot(qualityAnalysisResult, errorLimits = c(-0.15,0.15))
p %>% saveTwoThirdPageFigure(output("errorSummaryAnalysis"))
```

*Figure A2.5: Estimate quality metric intermediate detail and model comparison. Panel A-D show summary statistics for the bias of $R_t$ estimates broken down by the simulation smoothness, weekly variability, initial incidence, and time series boundary status. Panels E-F shows the calibration for the same subdivisions. Panels I-L shows the quantile density, panel M-P the CRPS, and panel Q-T the critical thresholds for the same subdivisions.*

At this intermediate detail the aggregation of the quality metrics are faceted by the salient features of the simulation, which inform us of where the estimators strengths and weaknesses are. These including the the smoothness of the growth rate function (first column), the degree of weekend effect (second column), initial size (third column), and also the proximity of the estimate to either end of the time series (final column). This last measure is defined as whether the estimate is within 21 days of the start or end of the time series. The latter is important because boundary effects particularly on most recent end of the time series may be important for real time estimates of $R_t$. This intermediate view of the quality of the estimates expands on the findings from figure A2.4, for example, in Panel G and K in Figure A2.5, we note the low calibration and excessive precision of all the estimates is due to the high incidence simulations. In panel O however we note that the CPRS is lower (better) in the higher incidence scenarios suggesting that the over-precise estimates do come with improved accuracy (also seen in panel C).

The boundary effect analysis in panels D,H,L,P demonstrates that all the methods investigated here are biased high in the first 3 weeks, and poorly calibrated. The critical threshold metric (panels Q,R,S & T) shows that the 14 day estimate method is able to detecting transitions between growth to decline well. The simulations with high weekly variation is seen to the accuracy of the 4 day estimate on this measure (panel R). The impact of the weekly variation on the 4 day estimate is also seen in panel B where increase spread of error occurs with increasing variation.

On the face of this comparison there is evidence to favour the 14 day estimate methodology.

To further understand the performance of the 14 day model, we can also visualize these metrics over time and between selected simulation configurations. For the single estimation methodology (14 days window) we can compare low and high incidence and smoothness of the simulation. We present in detail the 4 simulations with no weekend effects, 2 of which are based on smoothly varying growth rates, and 2 on stepped growth rates, with different numbers of initial cases (low incidence: 100 and high incidence: 10000).

```{r}
# # This summary is on a per model basis
# # Here we filter out the specific model
# tmp = syntheticEstimates2 %>% filter(model == "EpiEstim: 14 day" & weekendEffect == 0) %>% mutate(
#   label = paste0(smoothLabel,"; ",seedLabel)
# )
# tmp4 = tmp %>% ungroup() %>% select(label,comparison) %>% unnest(comparison)
# 
# 
# 
# # spread skill analysis at level of individual estimate
# # thsi is is variance versus square of residual essentially.
# # ggplot(tmp4,aes(x=Rt.SD^2,y=Rt.residual.sqrd, colour=lambda_t))+geom_point()+facet_wrap(vars(label))
# 
# # Rank histogram. Rank of actual versus bootstrap. Here as we have quantile we instead look at estimated quantile of actual.
# # Histogram shoudl be flat if confidence appropriate. If U shaped then too confident, if hill shaped then not confident enough.
# # ggplot(tmp4,aes(x=quantile.actual))+geom_density(bw=0.025)+facet_wrap(vars(label))
# 
# # The theoretical values:
# p1 = ggplot(tmp4 %>% filter(subgroup==1), aes(x=date)) +geom_point(aes(x=Rt.estimate.date,y=value),size=0.1)+ geom_line(aes(y=lambda_t), colour="red")+facet_wrap(vars(label),nrow = 1)+ylab(latex2exp::TeX("model cases"))+scale_y_continuous(trans="log1p",breaks = ukcovidtools::breaks_log1p())
# 
# p2 = ggplot(tmp4 %>% filter(subgroup==1), aes(x=date))+
#   geom_ribbon(aes(ymin=Rt.Lo, ymax=Rt.Hi),fill="grey80",colour=NA)+
#   geom_line(aes(y=Rt.Median),colour="black")+
#   geom_line(aes(y=Rt.actual), colour="red")+
#   facet_wrap(vars(label),nrow = 1)+ylab(latex2exp::TeX("model $R_t$"))+
#   coord_cartesian(ylim=c(0.6,1.4))
# 
# # ggplot(tmp4, aes(x=date,y=quantile.residual))+geom_point(size=0.5)+plotRollingQuantiles(stepCDF3 %>% ungroup(), quantile.residual,window = 28)+facet_wrap(vars(label))
# 
# # Timeseries of the comparison stats
# p3 = ggplot(tmp4, aes(x=date,y=Rt.residual))+geom_point(size=0.1,alpha=0.25)+plotRollingQuantiles(tmp4 %>% group_by(label), Rt.residual,window = 14)+coord_cartesian(ylim=c(-0.15,0.15)) + ylab("bias")+facet_wrap(vars(label),nrow = 1)
# p4 = ggplot(tmp4, aes(x=date,y=Rt.calibration))+plotRollingProportion(tmp4 %>% group_by(label), Rt.calibration,window = 7) + ylab("calibration")+facet_wrap(vars(label),nrow = 1)
# 
# 
# # ggplot(tmp4, aes(x=date,y=Rt.residual))+geom_point(size=0.1)+plotRollingQuantiles(stepCDF3 %>% ungroup(), Rt.residual,window = 28)+coord_cartesian(ylim=c(-0.2,0.2)) + ylab("residual error")+facet_wrap(vars(label))
# # ggplot(tmp4, aes(x=date,y=Rt.relative.percent))+geom_point(size=0.1)+plotRollingQuantiles(stepCDF3 %>% ungroup(), Rt.relative.percent,window = 28)+coord_cartesian(ylim=c(-15,15)) + ylab("relative error (%)")+facet_wrap(vars(label))
# # ggplot(tmp4, aes(x=date,y=Rt.residual.quantile))+geom_point(size=0.1)+plotRollingQuantiles(tmp4 %>% group_by(label), Rt.residual.quantile,window = 28)+ ylab("error quantile")+facet_wrap(vars(label))
# 
# # ggplot(tmp4, aes(x=date,y=Rt.residual.quantile))+geom_point(size=0.1)+plotRollingQuantiles(stepCDF3 %>% ungroup(), Rt.relative,window = 28)+coord_cartesian(ylim=c(0.875,1.125)) + ylab("relative error")+facet_wrap(vars(label))
# 
# #p4 = ggplot(tmp4, aes(x=date,y=quantile.actual))+geom_point(size=0.1)+plotRollingQuantiles(tmp4 %>% group_by(label), quantile.probability,window = 28) + ylab("Rank histogram")+facet_wrap(vars(label),nrow = 1)
# p5 = ggplot(tmp4, aes(x=date,y=quantile.actual))+
#   plotRollingDensity(tmp4 %>% group_by(label), quantile.actual, window = 14) + 
#   plotRollingDeciles(tmp4 %>% group_by(label), quantile.actual, window = 28) + 
#   ylab("quantile density")+
#   facet_wrap(vars(label),nrow = 1)+
#   scale_fill_gradient(low="white",high="black",guide = "none")
# 
# p6 = ggplot(tmp4, aes(x=date,y=Rt.crps))+geom_point(size=0.1,alpha=0.25)+plotRollingQuantiles(tmp4 %>% group_by(label), Rt.crps, window = 14) + ylab("instantaneous CRPS")+facet_wrap(vars(label),nrow = 1)
# p7 = ggplot(tmp4, aes(x=date,y=Rt.critical_threshold))+plotRollingProportion(tmp4 %>% group_by(label), Rt.critical_threshold,window = 7) + ylab("crit threshold")+facet_wrap(vars(label),nrow = 1)
# 
# # ggplot(tmp4, aes(x=date,y=Rt.residual.abs))+geom_point(size=0.5)+plotRollingQuantiles(stepCDF3 %>% ungroup(), Rt.residual.abs)+coord_cartesian(ylim=c(0,0.2))+facet_wrap(vars(label))
# 
# # printMedianAndCI(tmp4$Rt.residual)
# # 
# # printIQR(tmp4$quantile.probability)
# # printIQR(tmp4$Rt.residual.abs)
# 
# p = 
#   p1+standardPrintOutput::hideX()+
#   p2+standardPrintOutput::hideX()+
#   p3+standardPrintOutput::hideX()+
#   p4+standardPrintOutput::hideX()+
#   p5+standardPrintOutput::hideX()+
#   p6+standardPrintOutput::hideX()+
#   p7+
#   plot_annotation(tag_levels = "A")+plot_layout(ncol=1, heights = c(1,1,2,2,2,2,2))
p = estimateTimeseriesPlot(qualityAnalysisResult, simFilterExpr = weekendEffect==0, modelFilterExpr = model=="EpiEstim: 14 day", labelExpr = paste0(smoothLabel,"; ",seedLabel))
suppressWarnings(
  p %>% saveFullPageFigure(output("errorAnalysis"))
)


```

*Figure A2.6: Time variations in estimate quality metrics. Panel A shows the modelled case count, and one sample from the simulation for 4 synthetic data simulation configurations. Panel B shows the simulation reproduction number (red) and one estimate (black) based on one sample from the simulation, using the Cori method with a gamma distributed infectivity profile (mean 5 days, sd 4), and a fixed window of 14 days with the time corrected to account for estimate lags. Panel C shows the bias of each individual $R_t$ estimate and rolling quantiles over a 28 days period. Panel D shows the calibration of individual estimates and the rolling mean of the calibration (14 day window). Panel E shows the quantile density over time (with density as the shade), the rolling 10%, 30% 50% 70% and 90% quantiles are shown as grey lines calculated over a 28 day window. In panel F the instantaneous CRPS for individual estimates and the rolling quantiles are shown. In Panel G the critical threshold measure shows where the estimates confidence limits are the other side of the $R_t$ critical threshold of 1 to the actual value. In all panels quantiles shown in red are 2.5% and 97.5% if dotted, 25% and 75% if dashed and the median is a solid red line. Blue horizontal lines represent the median for the statistic for the whole time series.*

```{r}
tmp4 = qualityAnalysisResult %>% filter(weekendEffect == 0 & model == "EpiEstim: 14 day") %>% select(comparison) %>% unnest(comparison)
residualIQR = printIQR(tmp4$residual)
calibrationMean = sprintf("%1.0f%% [SD %1.0f%%]",mean(tmp4$calibration,na.rm = TRUE)*100,sd(tmp4$calibration,na.rm = TRUE)*100)
critThreshMean = sprintf("%1.2f%%",mean(tmp4$critical_threshold,na.rm = TRUE)*100)
# cat(residualIQR,calibrationMean,sep = "\n")
```

For this selection of simulation configurations and estimation method we again find that there is no evidence of strong systematic bias. Over all simulations in Figure A2.6 the bias is `r residualIQR`. These is a suggestion in Panel C that this varies over time, however the absolute bias remains small with the worst case being $\pm0.1$. The calibration of the estimates is very variable, and overall `r calibrationMean` of the estimates include the actual value in their confidence intervals. Panel D demonstrates that this is far worse when case numbers are high and during step changes in the reproduction number, which is due to over precision in the estimates. The quantile density (panel E) backs this up with the distribution swinging from one extreme to the other in all the parts of the simulations with high incidence, implying an over-precise estimate. However although the calibration is poor and the quantile density is far from a flat distribution, in the higher incidence simulations the bias of the estimates is smaller. When this is taken together, despite the over precision, in panel E the CRPS is in fact lowest for the higher incidence simulations. Overall we can conclude that that this estimation methodology is most accurate with high case numbers but produces slightly biased estimates with excessive confidence. It performs less well when there is an abrupt change in $R_t$ (panel G), and this means that `r critThreshMean` of the estimates using this method incorrectly predict growth when in fact the epidemic is in decline, or vice-versa. 

```{r}
# <!-- showing In Figure A2.4 panel C we see this is highest in the initial few data points due to a boundary effect, in the regions where overall incidence is very low (incidence curves are in panel A) and in the regions where there are abrupt transitions in theoretical $R_t$ (seen in Panel B). In the spline based examples there is more error during the period close to July when case numbers are low, and the theoretical $R_t$ value is close to 1. During this period there is an increase in absolute error which may be significant in practice as the detection of the transition to a growing epidemic when numbers are small is a key value of the reproduction number. -->
# 
# <!-- Panel D shows the likelihood of observing the theoretical value given the estimate, and demonstrates that just over half (`r printIQR(stepCDF3$quantile.probability)`) of the estimates include the theoretical value within their intra quartile range. There is a notable dip in the spline based examples in October, which corresponds to the period when incidence was high and $R_t$ changing. As seen on the equivalent panel in figure A2.2 during this period the estimates are quite precise but generally slightly inaccurate. This leads to a dip in the probability of the theoretical value being within the estimates confidence intervals and is a sign of excessive confidence in the estimates during this period. The same effect is seen during the early part of all time series due to the boundary effect. -->
# 
# <!-- The quantile of the theoretical value measured on the distribution of the $R_t$ estimate is equivalent to a prediction of the quantile of the absolute residual value. We can also observe the quantile of the absolute residual error between individual estimates and their associated theoretical values. If the estimate uncertainty is representative the predicted quantile should match the observed quantile of the residual and in an estimate with a well calibrated uncertainty these two values should be proportional. In Figure A2.5 this relationship is plotted for all the estimates in all the simulations, with colour representing the incidence when the estimate was made. There is a clear difference between the certainty of estimates made when incidence is high compared to those made when it is low. During periods of high incidence the estimates are more likely to have a lower observed residual quantile and higher predicted quantile for the residual. This demonstrates the estimate is more precise when case numbers are high and that this precision is leading to higher degree of estimation error. -->
# 
# <!-- ```{r} -->
# <!-- # ggplot(tmp4, aes(x=Rt.residual.abs, y=quantile.probability,colour=lambda_t))+geom_point(size=0.1)+coord_cartesian(xlim=c(0,0.15))+scale_colour_viridis_c(name="incidence")+xlab(latex2exp::TeX("$R_t$ | residual error|"))+ylab(latex2exp::TeX("$R_t$ P(theoretical | estimate)")) -->
# <!-- p = ggplot(tmp4, aes(x=Rt.residual.quantile, y=1-quantile.probability,colour=lambda_t))+geom_point(size=0.1)+geom_abline(slope=1, colour="red")+scale_colour_viridis_c(name="incidence")+xlab("Observed residual quantile")+ylab("Predicted residual quantile")+coord_fixed() -->
# <!-- p %>% standardPrintOutput::saveSixthPageFigure(output("QQplot")) -->
# <!-- ``` -->
# <!-- *Figure A2.5: For each individual $R_t$ estimate we compare the observed quantiles of the residual error, to the predicted quantiles of the residual based on the confidence intervals of the $R_t$ estimate and the theoretical value. The red line shows the ideal relationship.* -->
```

# Summary

This paper describes a methodology for verifying the estimation of $R_t$ using synthetic data sets designed to highlight particular issues, by comparing estimates to known $R_t$ values. The performance of the estimation methods can be summarized with 5 metrics, the bias, calibration, quantile density and continuous ranked probability score, and the critical threshold measure. At a summary level the combination of these metrics allows performance of different estimate methods to be quantitatively compared. Furthermore by comparing performance of different estimation methods against different simulations with specific features, we can qualitatively assess situations in which individual estimation methodologies perform better or worse. A detailed analysis of the time series of the metrics gives us further insight into situations where a given estimate method may under-perform. 

This validation methodology is extensible to other metrics other than $R_t$ where a set of confidence intervals or quantiles are available, particularly estimates of growth rate and incidence.